{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ff51548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12-09 17:42:01 [INFO] llm_utils.local_client - Model validation successful: ./models/qwen3-4b-thinking\n",
      "12-09 17:42:01 [INFO] llm_utils.local_client - Model found and validated at local path: ./models/qwen3-4b-thinking\n",
      "12-09 17:42:03 [INFO] llm_utils.local_client - Loading tokenizer from: ./models/qwen3-4b-thinking\n",
      "12-09 17:42:04 [INFO] llm_utils.local_client - Loading model from: ./models/qwen3-4b-thinking\n",
      "12-09 17:42:04 [INFO] llm_utils.local_client - Device: auto, Dtype: torch.float16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "432d0f5ad6434ca495f82094e42fcd33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12-09 17:42:07 [INFO] llm_utils.local_client - Local LLM client initialized: Qwen/Qwen3-4B-Thinking-2507\n",
      "开始因果推断分析（使用本地白盒模型 + token概率）\n",
      "============================================================\n",
      "\n",
      "使用方法: token_prob\n",
      "----------------------------------------\n",
      "=== Step 1: 检查后门路径 ===\n",
      "12-09 17:42:07 [INFO] llm_utils.local_client - Model validation successful: ./models/qwen3-4b-thinking\n",
      "12-09 17:42:07 [INFO] llm_utils.local_client - Model found and validated at local path: ./models/qwen3-4b-thinking\n",
      "12-09 17:42:07 [INFO] llm_utils.local_client - Loading tokenizer from: ./models/qwen3-4b-thinking\n",
      "12-09 17:42:07 [INFO] llm_utils.local_client - Loading model from: ./models/qwen3-4b-thinking\n",
      "12-09 17:42:07 [INFO] llm_utils.local_client - Device: auto, Dtype: torch.float16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cca3f40aaec4bc7b530687e6aaa1fbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12-09 17:42:11 [INFO] llm_utils.local_client - Local LLM client initialized: Qwen/Qwen3-4B-Thinking-2507\n",
      "门诊agent原始响应: 首先，任务是分析变量对：冰淇淋销量 和 溺水人数。分析类型是 backdoor_path，这在因果推断中指的是后门路径，用于处理混淆变量。\n",
      "\n",
      "我需要从可用专家列表中选择最相关的3个专家，按相关性从高到低排序。专家列表包括：\n",
      "- graph_theory\n",
      "- statistical\n",
      "- counterfactual\n",
      "- temporal_dynamics\n",
      "- mechanism_modeling\n",
      "- robustness_analysis\n",
      "- domain_knowledge\n",
      "\n",
      "选择要求：\n",
      "1. 根据变量内容和问题类型：变量是冰淇淋销量（冰淇淋销售）和溺水人数（溺水事件）。问题类型是 backdoor_path，这涉及因果图中的后门准则，用于识别和控制混淆变量。\n",
      "2. 按相关性从高到低排序。\n",
      "3. 确保专家视角的多样性：不要选择推理方式相似的专家。\n",
      "4. 考虑变量的领域特性：冰淇淋销量和\n",
      "门诊agent推荐: ['graph_theory', 'statistical', 'counterfactual']\n",
      "为问题 'backdoor_path' 选择的专家: ['graph_theory', 'statistical', 'counterfactual']\n",
      "  采样1: P(Yes)=0.5000, P(No)=0.5000\n",
      "  采样2: P(Yes)=0.5000, P(No)=0.5000\n",
      "  采样3: P(Yes)=0.5000, P(No)=0.5000\n",
      "  平均概率: P(Yes)=0.5000, P(No)=0.5000\n",
      "专家 graph_theory 判断完成: label=1, P(Yes)=0.5000, P(No)=0.5000\n",
      "  采样1: P(Yes)=0.5000, P(No)=0.5000\n",
      "  采样2: P(Yes)=0.5000, P(No)=0.5000\n",
      "  采样3: P(Yes)=0.5000, P(No)=0.5000\n",
      "  平均概率: P(Yes)=0.5000, P(No)=0.5000\n",
      "专家 statistical 判断完成: label=1, P(Yes)=0.5000, P(No)=0.5000\n",
      "  采样1: P(Yes)=0.5000, P(No)=0.5000\n",
      "  采样2: P(Yes)=0.5000, P(No)=0.5000\n",
      "  采样3: P(Yes)=0.5000, P(No)=0.5000\n",
      "  平均概率: P(Yes)=0.5000, P(No)=0.5000\n",
      "专家 counterfactual 判断完成: label=1, P(Yes)=0.5000, P(No)=0.5000\n",
      "专家整合结果: label=1, P(Yes)=0.5000, P(No)=0.5000\n",
      "存在后门路径，进入阻断后分析路径\n",
      "=== Step 2: 阻断后检查独立性 ===\n",
      "12-09 17:44:27 [INFO] llm_utils.local_client - Model validation successful: ./models/qwen3-4b-thinking\n",
      "12-09 17:44:27 [INFO] llm_utils.local_client - Model found and validated at local path: ./models/qwen3-4b-thinking\n",
      "12-09 17:44:27 [INFO] llm_utils.local_client - Loading tokenizer from: ./models/qwen3-4b-thinking\n",
      "12-09 17:44:27 [INFO] llm_utils.local_client - Loading model from: ./models/qwen3-4b-thinking\n",
      "12-09 17:44:27 [INFO] llm_utils.local_client - Device: auto, Dtype: torch.float16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "177e6474469848748b82aa7e2b6e2b7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12-09 17:44:31 [INFO] llm_utils.local_client - Local LLM client initialized: Qwen/Qwen3-4B-Thinking-2507\n",
      "门诊agent原始响应: 首先，任务是分析变量对：冰淇淋销量 和 溺水人数。问题类型是 \"independence\"，意思是独立性分析。我需要选择最合适的3个专家组合，从给定的列表中。\n",
      "\n",
      "回顾专家列表：\n",
      "\n",
      "- statistical: 专注于统计检验和概率独立性分析，擅长相关性分析、条件独立性检验、混淆变量检测。推理方式：考虑样本分布、统计显著性、置信区间等。\n",
      "\n",
      "- graph_theory: 专门分析因果图结构，精通d-分离、路径阻断、后门准则等图论概念。\n",
      "\n",
      "- counterfactual: 从干预和潜在结果角度分析因果关系，考虑do-calculus、随机化实验理想情况。\n",
      "\n",
      "- robustness_analysis: 专门评估因果关系的稳健性和敏感性，考虑不同假设下的结果稳定性。\n",
      "\n",
      "- temporal_dynamics: 专门分析时间顺序和动态过程，强调原因必须发生在结果之前，考虑延迟效应和动态反馈\n",
      "门诊agent推荐: ['statistical', 'graph_theory', 'counterfactual']\n",
      "为问题 'independence' 选择的专家: ['statistical', 'graph_theory', 'counterfactual']\n",
      "  采样1: P(Yes)=0.5000, P(No)=0.5000\n",
      "  采样2: P(Yes)=0.5000, P(No)=0.5000\n",
      "  采样3: P(Yes)=0.5000, P(No)=0.5000\n",
      "  平均概率: P(Yes)=0.5000, P(No)=0.5000\n",
      "专家 statistical 判断完成: label=1, P(Yes)=0.5000, P(No)=0.5000\n",
      "  采样1: P(Yes)=0.5000, P(No)=0.5000\n",
      "  采样2: P(Yes)=0.5000, P(No)=0.5000\n",
      "  采样3: P(Yes)=0.5000, P(No)=0.5000\n",
      "  平均概率: P(Yes)=0.5000, P(No)=0.5000\n",
      "专家 graph_theory 判断完成: label=1, P(Yes)=0.5000, P(No)=0.5000\n",
      "  采样1: P(Yes)=0.5000, P(No)=0.5000\n",
      "  采样2: P(Yes)=0.5000, P(No)=0.5000\n",
      "  采样3: P(Yes)=0.5000, P(No)=0.5000\n",
      "  平均概率: P(Yes)=0.5000, P(No)=0.5000\n",
      "专家 counterfactual 判断完成: label=1, P(Yes)=0.5000, P(No)=0.5000\n",
      "专家整合结果: label=1, P(Yes)=0.5000, P(No)=0.5000\n",
      "\n",
      "=== token_prob方法结果 ===\n",
      "变量关系: independent\n",
      "置信度 (P(Yes)): 0.5000\n",
      "\n",
      "=== 详细执行日志 ===\n",
      "\n",
      "backdoor_path:\n",
      "  最终判断: Yes\n",
      "  P(Yes): 0.5000\n",
      "  P(No): 0.5000\n",
      "  专家详情:\n",
      "    - graph_theory: Yes (P(Yes)=0.5000, P(No)=0.5000)\n",
      "    - statistical: Yes (P(Yes)=0.5000, P(No)=0.5000)\n",
      "    - counterfactual: Yes (P(Yes)=0.5000, P(No)=0.5000)\n",
      "\n",
      "independent_after_block:\n",
      "  最终判断: Yes\n",
      "  P(Yes): 0.5000\n",
      "  P(No): 0.5000\n",
      "  专家详情:\n",
      "    - statistical: Yes (P(Yes)=0.5000, P(No)=0.5000)\n",
      "    - graph_theory: Yes (P(Yes)=0.5000, P(No)=0.5000)\n",
      "    - counterfactual: Yes (P(Yes)=0.5000, P(No)=0.5000)\n",
      "\n",
      "使用方法: frequency\n",
      "----------------------------------------\n",
      "=== Step 1: 检查后门路径 ===\n",
      "12-09 17:46:52 [INFO] llm_utils.local_client - Model validation successful: ./models/qwen3-4b-thinking\n",
      "12-09 17:46:52 [INFO] llm_utils.local_client - Model found and validated at local path: ./models/qwen3-4b-thinking\n",
      "12-09 17:46:52 [INFO] llm_utils.local_client - Loading tokenizer from: ./models/qwen3-4b-thinking\n",
      "12-09 17:46:52 [INFO] llm_utils.local_client - Loading model from: ./models/qwen3-4b-thinking\n",
      "12-09 17:46:52 [INFO] llm_utils.local_client - Device: auto, Dtype: torch.float16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bec9c3d43584493ba25f396d5b56635e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12-09 17:46:56 [INFO] llm_utils.local_client - Local LLM client initialized: Qwen/Qwen3-4B-Thinking-2507\n",
      "门诊agent原始响应: 首先，任务是分析变量对：冰淇淋销量 和 溺水人数。分析类型是 backdoor_path，这在因果推断中指的是后门路径，用于处理混淆变量。\n",
      "\n",
      "我需要从可用专家列表中选择最相关的3个专家，按相关性从高到低排序。专家列表包括：\n",
      "- graph_theory\n",
      "- statistical\n",
      "- counterfactual\n",
      "- temporal_dynamics\n",
      "- mechanism_modeling\n",
      "- robustness_analysis\n",
      "- domain_knowledge\n",
      "\n",
      "选择要求：\n",
      "1. 根据变量内容和问题类型，选择最相关的3个专家。\n",
      "2. 按相关性从高到低排序。\n",
      "3. 确保专家视角的多样性（不要选择推理方式相似的专家）。\n",
      "4. 考虑变量的领域特性：冰淇淋销量和溺水人数是社会/经济领域，涉及天气、季节等。\n",
      "\n",
      "变量内容：冰淇淋销量（可能受温度、季节影响）和溺水人数\n",
      "门诊agent推荐: ['graph_theory', 'statistical', 'counterfactual']\n",
      "为问题 'backdoor_path' 选择的专家: ['graph_theory', 'statistical', 'counterfactual']\n",
      "专家 graph_theory 判断完成: label=1, P(Yes)=0.5000, P(No)=0.5000\n",
      "专家 statistical 判断完成: label=1, P(Yes)=0.5000, P(No)=0.5000\n",
      "专家 counterfactual 判断完成: label=1, P(Yes)=0.5000, P(No)=0.5000\n",
      "专家整合结果: label=1, P(Yes)=0.5000, P(No)=0.5000\n",
      "存在后门路径，进入阻断后分析路径\n",
      "=== Step 2: 阻断后检查独立性 ===\n",
      "12-09 17:49:11 [INFO] llm_utils.local_client - Model validation successful: ./models/qwen3-4b-thinking\n",
      "12-09 17:49:11 [INFO] llm_utils.local_client - Model found and validated at local path: ./models/qwen3-4b-thinking\n",
      "12-09 17:49:11 [INFO] llm_utils.local_client - Loading tokenizer from: ./models/qwen3-4b-thinking\n",
      "12-09 17:49:11 [INFO] llm_utils.local_client - Loading model from: ./models/qwen3-4b-thinking\n",
      "12-09 17:49:11 [INFO] llm_utils.local_client - Device: auto, Dtype: torch.float16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77eecc60e390407bbe7432d0d3fd130a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12-09 17:49:14 [INFO] llm_utils.local_client - Local LLM client initialized: Qwen/Qwen3-4B-Thinking-2507\n",
      "门诊agent原始响应: 首先，任务是分析变量对：冰淇淋销量 和 溺水人数。问题类型是 \"independence\"，意思是独立性分析。我需要选择最合适的3个专家组合，从给定的列表中。\n",
      "\n",
      "回顾专家列表：\n",
      "\n",
      "- statistical: 专注于统计检验和概率独立性分析，擅长相关性分析、条件独立性检验、混淆变量检测。推理方式：考虑样本分布、统计显著性、置信区间等。\n",
      "\n",
      "- graph_theory: 专门分析因果图结构，精通d-分离、路径阻断、后门准则等图论概念。\n",
      "\n",
      "- counterfactual: 从干预和潜在结果角度分析因果关系，考虑do-calculus、随机化实验理想情况。\n",
      "\n",
      "- robustness_analysis: 专门评估因果关系的稳健性和敏感性，考虑不同假设下的结果稳定性。\n",
      "\n",
      "- temporal_dynamics: 专门分析时间顺序和动态过程，强调原因必须发生在结果之前，考虑延迟效应和动态反馈\n",
      "门诊agent推荐: ['statistical', 'graph_theory', 'counterfactual']\n",
      "为问题 'independence' 选择的专家: ['statistical', 'graph_theory', 'counterfactual']\n",
      "专家 statistical 判断完成: label=1, P(Yes)=0.5000, P(No)=0.5000\n",
      "专家 graph_theory 判断完成: label=1, P(Yes)=0.5000, P(No)=0.5000\n",
      "专家 counterfactual 判断完成: label=1, P(Yes)=0.5000, P(No)=0.5000\n",
      "专家整合结果: label=1, P(Yes)=0.5000, P(No)=0.5000\n",
      "\n",
      "=== frequency方法结果 ===\n",
      "变量关系: independent\n",
      "置信度 (P(Yes)): 0.5000\n",
      "\n",
      "=== 详细执行日志 ===\n",
      "\n",
      "backdoor_path:\n",
      "  最终判断: Yes\n",
      "  P(Yes): 0.5000\n",
      "  P(No): 0.5000\n",
      "  专家详情:\n",
      "    - graph_theory: Yes (P(Yes)=0.5000, P(No)=0.5000)\n",
      "    - statistical: Yes (P(Yes)=0.5000, P(No)=0.5000)\n",
      "    - counterfactual: Yes (P(Yes)=0.5000, P(No)=0.5000)\n",
      "\n",
      "independent_after_block:\n",
      "  最终判断: Yes\n",
      "  P(Yes): 0.5000\n",
      "  P(No): 0.5000\n",
      "  专家详情:\n",
      "    - statistical: Yes (P(Yes)=0.5000, P(No)=0.5000)\n",
      "    - graph_theory: Yes (P(Yes)=0.5000, P(No)=0.5000)\n",
      "    - counterfactual: Yes (P(Yes)=0.5000, P(No)=0.5000)\n",
      "\n",
      "使用方法: logit\n",
      "----------------------------------------\n",
      "=== Step 1: 检查后门路径 ===\n",
      "12-09 17:51:27 [INFO] llm_utils.local_client - Model validation successful: ./models/qwen3-4b-thinking\n",
      "12-09 17:51:27 [INFO] llm_utils.local_client - Model found and validated at local path: ./models/qwen3-4b-thinking\n",
      "12-09 17:51:27 [INFO] llm_utils.local_client - Loading tokenizer from: ./models/qwen3-4b-thinking\n",
      "12-09 17:51:27 [INFO] llm_utils.local_client - Loading model from: ./models/qwen3-4b-thinking\n",
      "12-09 17:51:27 [INFO] llm_utils.local_client - Device: auto, Dtype: torch.float16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85c43243a9214153b57ac00d38749fc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12-09 17:51:30 [INFO] llm_utils.local_client - Local LLM client initialized: Qwen/Qwen3-4B-Thinking-2507\n",
      "门诊agent原始响应: 首先，任务是分析变量对：冰淇淋销量 和 溺水人数。分析类型是 backdoor_path，这在因果推断中指的是后门路径（backdoor path）的分析，用于处理混淆变量。\n",
      "\n",
      "关键点是：冰淇淋销量和溺水人数。我需要回想一下，冰淇淋销量和溺水人数之间有什么因果关系？在现实中，冰淇淋销量和溺水人数通常有正相关，因为夏天热的时候，人们吃更多冰淇淋，同时游泳更多，溺水事件也增多。但这里没有直接的因果；它们可能都受天气（如温度）的影响。所以，天气是混淆变量。\n",
      "\n",
      "在因果推断中，后门路径分析用于当存在一个共同原因（混淆变量）时，如何估计因果效应。例如，想从冰淇淋销量（原因）到溺水人数（结果）的因果效应，但需要控制混淆变量。\n",
      "\n",
      "现在，我需要从可用专家列表中选择最相关的3个\n",
      "门诊agent解析不充分，使用基础专家: ['graph_theory', 'statistical', 'counterfactual']\n",
      "门诊agent推荐: ['graph_theory', 'statistical', 'counterfactual']\n",
      "为问题 'backdoor_path' 选择的专家: ['graph_theory', 'statistical', 'counterfactual']\n",
      "专家 graph_theory 判断完成: label=1, P(Yes)=0.5000, P(No)=0.5000\n",
      "专家 statistical 判断完成: label=1, P(Yes)=0.5000, P(No)=0.5000\n",
      "专家 counterfactual 判断完成: label=1, P(Yes)=0.5000, P(No)=0.5000\n",
      "专家整合结果: label=1, P(Yes)=0.5000, P(No)=0.5000\n",
      "存在后门路径，进入阻断后分析路径\n",
      "=== Step 2: 阻断后检查独立性 ===\n",
      "12-09 17:53:07 [INFO] llm_utils.local_client - Model validation successful: ./models/qwen3-4b-thinking\n",
      "12-09 17:53:07 [INFO] llm_utils.local_client - Model found and validated at local path: ./models/qwen3-4b-thinking\n",
      "12-09 17:53:07 [INFO] llm_utils.local_client - Loading tokenizer from: ./models/qwen3-4b-thinking\n",
      "12-09 17:53:07 [INFO] llm_utils.local_client - Loading model from: ./models/qwen3-4b-thinking\n",
      "12-09 17:53:07 [INFO] llm_utils.local_client - Device: auto, Dtype: torch.float16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5c9c7bef8ae4f1ba05f90759e424471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12-09 17:53:11 [INFO] llm_utils.local_client - Local LLM client initialized: Qwen/Qwen3-4B-Thinking-2507\n",
      "门诊agent原始响应: 首先，任务是分析变量对：冰淇淋销量 和 溺水人数。问题类型是 \"independence\"，意思是独立性分析。我需要选择最合适的3个专家组合，从给定的列表中。\n",
      "\n",
      "可用专家列表：\n",
      "- statistical: 专注于统计检验和概率独立性分析，擅长相关性分析、条件独立性检验、混淆变量检测。推理方式：考虑样本分布、统计显著性、置信区间等。\n",
      "- graph_theory: 专门分析因果图结构，精通d-分离、路径阻断、后门准则等图论概念。\n",
      "- counterfactual: 从干预和潜在结果角度分析因果关系，考虑do-calculus、随机化实验理想情况。\n",
      "- robustness_analysis: 专门评估因果关系的稳健性和敏感性，考虑不同假设下的结果稳定性。\n",
      "- temporal_dynamics: 专门分析时间顺序和动态过程，强调原因必须发生在结果之前，考虑延迟效应和动态反馈\n",
      "门诊agent推荐: ['statistical', 'graph_theory', 'counterfactual']\n",
      "为问题 'independence' 选择的专家: ['statistical', 'graph_theory', 'counterfactual']\n",
      "专家 statistical 判断完成: label=1, P(Yes)=0.5000, P(No)=0.5000\n",
      "专家 graph_theory 判断完成: label=1, P(Yes)=0.5000, P(No)=0.5000\n",
      "专家 counterfactual 判断完成: label=1, P(Yes)=0.5000, P(No)=0.5000\n",
      "专家整合结果: label=1, P(Yes)=0.5000, P(No)=0.5000\n",
      "\n",
      "=== logit方法结果 ===\n",
      "变量关系: independent\n",
      "置信度 (P(Yes)): 0.5000\n",
      "\n",
      "=== 详细执行日志 ===\n",
      "\n",
      "backdoor_path:\n",
      "  最终判断: Yes\n",
      "  P(Yes): 0.5000\n",
      "  P(No): 0.5000\n",
      "  专家详情:\n",
      "    - graph_theory: Yes (P(Yes)=0.5000, P(No)=0.5000)\n",
      "    - statistical: Yes (P(Yes)=0.5000, P(No)=0.5000)\n",
      "    - counterfactual: Yes (P(Yes)=0.5000, P(No)=0.5000)\n",
      "\n",
      "independent_after_block:\n",
      "  最终判断: Yes\n",
      "  P(Yes): 0.5000\n",
      "  P(No): 0.5000\n",
      "  专家详情:\n",
      "    - statistical: Yes (P(Yes)=0.5000, P(No)=0.5000)\n",
      "    - graph_theory: Yes (P(Yes)=0.5000, P(No)=0.5000)\n",
      "    - counterfactual: Yes (P(Yes)=0.5000, P(No)=0.5000)\n",
      "\n",
      "============================================================\n",
      "=== 不同方法结果比较 ===\n",
      "token_prob: 关系=independent, 置信度=0.5000\n",
      "frequency: 关系=independent, 置信度=0.5000\n",
      "logit: 关系=independent, 置信度=0.5000\n",
      "\n",
      "============================================================\n",
      "12-09 17:54:43 [INFO] llm_utils.local_client - Model unloaded: Qwen/Qwen3-4B-Thinking-2507\n",
      "模型已卸载\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import re\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from llm_utils import LocalLLMClient, setup_logging\n",
    "\n",
    "# ============== 配置 ==============\n",
    "setup_logging(level=\"INFO\")\n",
    "\n",
    "# 创建全局本地LLM客户端（白盒模型）\n",
    "client = LocalLLMClient(\n",
    "    model_id=\"Qwen/Qwen3-4B-Thinking-2507\",  # 本地模型\n",
    "    local_path=\"./models/qwen3-4b-thinking\",\n",
    "    device=\"auto\",  # 自动选择GPU/CPU\n",
    "    max_tokens=200,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# === 工具函数 ===\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "def get_yes_no_probabilities_from_distribution(token_distribution: Dict[str, float]) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    从token概率分布中提取Yes/No的概率\n",
    "    \"\"\"\n",
    "    # 初始化概率\n",
    "    yes_prob = 0.0\n",
    "    no_prob = 0.0\n",
    "    \n",
    "    # 查找各种形式的Yes/No token\n",
    "    yes_tokens = [\"Yes\", \"yes\", \"YES\", \"是\", \"对的\", \"正确\", \"True\", \"true\"]\n",
    "    no_tokens = [\"No\", \"no\", \"NO\", \"否\", \"不对\", \"错误\", \"False\", \"false\"]\n",
    "    \n",
    "    # 累加所有Yes相关token的概率\n",
    "    for token in yes_tokens:\n",
    "        if token in token_distribution:\n",
    "            yes_prob += token_distribution[token]\n",
    "    \n",
    "    # 累加所有No相关token的概率\n",
    "    for token in no_tokens:\n",
    "        if token in token_distribution:\n",
    "            no_prob += token_distribution[token]\n",
    "    \n",
    "    # 如果没有找到任何Yes/No token，检查子串\n",
    "    if yes_prob == 0 and no_prob == 0:\n",
    "        for token, prob in token_distribution.items():\n",
    "            token_lower = token.lower()\n",
    "            if any(yes_word in token_lower for yes_word in [\"yes\", \"是\", \"对\", \"true\"]):\n",
    "                yes_prob += prob\n",
    "            elif any(no_word in token_lower for no_word in [\"no\", \"否\", \"错\", \"false\"]):\n",
    "                no_prob += prob\n",
    "    \n",
    "    # 如果还是没有找到，尝试归一化第一个token的概率\n",
    "    if yes_prob == 0 and no_prob == 0 and token_distribution:\n",
    "        # 取前两个token的概率\n",
    "        sorted_tokens = sorted(token_distribution.items(), key=lambda x: x[1], reverse=True)\n",
    "        if len(sorted_tokens) >= 2:\n",
    "            # 假设第一个token是\"Yes\"或\"No\"\n",
    "            # 这里我们无法判断，返回0.5/0.5\n",
    "            return 0.5, 0.5\n",
    "    \n",
    "    # 归一化概率\n",
    "    total = yes_prob + no_prob\n",
    "    if total > 0:\n",
    "        yes_prob_normalized = yes_prob / total\n",
    "        no_prob_normalized = no_prob / total\n",
    "    else:\n",
    "        yes_prob_normalized = 0.5\n",
    "        no_prob_normalized = 0.5\n",
    "    \n",
    "    return yes_prob_normalized, no_prob_normalized\n",
    "\n",
    "def llm_judge_with_token_prob(prompt, x1, x2, n_sample=5, method='token_prob'):\n",
    "    \"\"\"\n",
    "    使用白盒LLM进行判断，基于token概率\n",
    "    \n",
    "    参数:\n",
    "        prompt: 提示词\n",
    "        x1, x2: 变量名\n",
    "        n_sample: 采样次数（用于frequency模拟）\n",
    "        method: 'token_prob' | 'frequency' | 'logit' (保持接口兼容性，实际都使用token概率)\n",
    "    \"\"\"\n",
    "    if method == 'frequency':\n",
    "        # 频率方法：多次采样，统计Yes/No出现频率\n",
    "        votes = []\n",
    "        yes_probs = []\n",
    "        no_probs = []\n",
    "        \n",
    "        for _ in range(n_sample):\n",
    "            try:\n",
    "                # 调用本地模型，获取token概率\n",
    "                response = client.chat(\n",
    "                    prompt,\n",
    "                    return_token_probs=True,\n",
    "                    temperature=0.7  # 添加随机性\n",
    "                )\n",
    "                \n",
    "                # 裁剪思考链\n",
    "                cropped_response = response.crop_thinking()\n",
    "                \n",
    "                # 获取第一个token的概率分布\n",
    "                token_dist = client.get_token_distributions(\n",
    "                    cropped_response,\n",
    "                    top_k=20,\n",
    "                    skip_thinking=True,\n",
    "                    skip_zeros=True,\n",
    "                    zero_threshold=0.001\n",
    "                )\n",
    "                \n",
    "                if len(token_dist) > 0:\n",
    "                    # 从概率分布中提取Yes/No概率\n",
    "                    yes_prob, no_prob = get_yes_no_probabilities_from_distribution(token_dist[0])\n",
    "                    yes_probs.append(yes_prob)\n",
    "                    no_probs.append(no_prob)\n",
    "                    \n",
    "                    # 根据概率投票\n",
    "                    vote = 1 if yes_prob >= no_prob else 0\n",
    "                    votes.append(vote)\n",
    "            except Exception as e:\n",
    "                print(f\"采样失败: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not votes:\n",
    "            p_yes = 0.5\n",
    "            p_no = 0.5\n",
    "        else:\n",
    "            p_yes = sum(votes) / len(votes)\n",
    "            p_no = 1 - p_yes\n",
    "        \n",
    "        # 计算平均概率\n",
    "        avg_yes_prob = sum(yes_probs) / len(yes_probs) if yes_probs else p_yes\n",
    "        avg_no_prob = sum(no_probs) / len(no_probs) if no_probs else p_no\n",
    "        \n",
    "        if p_yes >= p_no:\n",
    "            return {\"label\": 1, \"prob\": avg_yes_prob, \"yes_prob\": avg_yes_prob, \"no_prob\": avg_no_prob}\n",
    "        else:\n",
    "            return {\"label\": 0, \"prob\": avg_no_prob, \"yes_prob\": avg_yes_prob, \"no_prob\": avg_no_prob}\n",
    "    \n",
    "    elif method == 'logit':\n",
    "        # Logit方法：解析logits并计算概率\n",
    "        try:\n",
    "            response = client.chat(prompt, return_token_probs=True)\n",
    "            text = response.content.strip()\n",
    "            \n",
    "            # 尝试从文本中解析logits\n",
    "            matches = re.findall(r\"([-]?[0-9]*\\.?[0-9]+)\", text)\n",
    "            if len(matches) >= 2:\n",
    "                l1, l2 = float(matches[0]), float(matches[1])\n",
    "                logit_yes, logit_no = l1, l2\n",
    "            else:\n",
    "                # 如果无法解析，使用token概率\n",
    "                return llm_judge_with_token_prob(prompt, x1, x2, n_sample=1, method='token_prob')\n",
    "            \n",
    "            p_yes = sigmoid(logit_yes)\n",
    "            p_no = sigmoid(logit_no)\n",
    "            \n",
    "            # 归一化\n",
    "            total = p_yes + p_no\n",
    "            if total > 0:\n",
    "                p_yes_normalized = p_yes / total\n",
    "                p_no_normalized = p_no / total\n",
    "            else:\n",
    "                p_yes_normalized = 0.5\n",
    "                p_no_normalized = 0.5\n",
    "            \n",
    "            if p_yes_normalized >= p_no_normalized:\n",
    "                return {\"label\": 1, \"prob\": p_yes_normalized, \"yes_prob\": p_yes_normalized, \"no_prob\": p_no_normalized}\n",
    "            else:\n",
    "                return {\"label\": 0, \"prob\": p_no_normalized, \"yes_prob\": p_yes_normalized, \"no_prob\": p_no_normalized}\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Logit方法失败: {e}, 回退到token_prob方法\")\n",
    "            return llm_judge_with_token_prob(prompt, x1, x2, n_sample=1, method='token_prob')\n",
    "    \n",
    "    else:  # token_prob 方法\n",
    "        # 使用白盒模型的token概率\n",
    "        all_yes_probs = []\n",
    "        all_no_probs = []\n",
    "        \n",
    "        for i in range(max(1, n_sample)):  # 至少采样1次\n",
    "            try:\n",
    "                # 调用本地模型，获取token概率\n",
    "                response = client.chat(\n",
    "                    prompt,\n",
    "                    return_token_probs=True,\n",
    "                    temperature=0.1 if n_sample == 1 else 0.7  # 单次采样降低随机性\n",
    "                )\n",
    "                \n",
    "                # 裁剪思考链\n",
    "                cropped_response = response.crop_thinking()\n",
    "                \n",
    "                # 获取第一个token的概率分布\n",
    "                token_dist = client.get_token_distributions(\n",
    "                    cropped_response,\n",
    "                    top_k=20,\n",
    "                    skip_thinking=True,\n",
    "                    skip_zeros=True,\n",
    "                    zero_threshold=0.001\n",
    "                )\n",
    "                \n",
    "                if len(token_dist) > 0:\n",
    "                    # 从概率分布中提取Yes/No概率\n",
    "                    yes_prob, no_prob = get_yes_no_probabilities_from_distribution(token_dist[0])\n",
    "                    \n",
    "                    all_yes_probs.append(yes_prob)\n",
    "                    all_no_probs.append(no_prob)\n",
    "                    \n",
    "                    if n_sample > 1:\n",
    "                        print(f\"  采样{i+1}: P(Yes)={yes_prob:.4f}, P(No)={no_prob:.4f}\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"采样{i+1}失败: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # 如果没有成功采样，使用默认值\n",
    "        if not all_yes_probs:\n",
    "            avg_yes_prob = 0.5\n",
    "            avg_no_prob = 0.5\n",
    "        else:\n",
    "            # 计算平均概率\n",
    "            avg_yes_prob = sum(all_yes_probs) / len(all_yes_probs)\n",
    "            avg_no_prob = sum(all_no_probs) / len(all_no_probs)\n",
    "        \n",
    "        if n_sample > 1:\n",
    "            print(f\"  平均概率: P(Yes)={avg_yes_prob:.4f}, P(No)={avg_no_prob:.4f}\")\n",
    "        \n",
    "        # 根据平均概率决定标签\n",
    "        if avg_yes_prob >= avg_no_prob:\n",
    "            return {\"label\": 1, \"prob\": avg_yes_prob, \"yes_prob\": avg_yes_prob, \"no_prob\": avg_no_prob}\n",
    "        else:\n",
    "            return {\"label\": 0, \"prob\": avg_no_prob, \"yes_prob\": avg_yes_prob, \"no_prob\": avg_no_prob}\n",
    "\n",
    "# === MoE 专家定义 ===\n",
    "CAUSAL_EXPERTS = {\n",
    "    \"graph_theory\": {\n",
    "        \"name\": \"因果图论专家\",\n",
    "        \"description\": \"专门分析因果图结构，精通d-分离、路径阻断、后门准则等图论概念。推理方式：系统检查所有可能的路径，分析路径上的变量类型和连接方式，使用严谨的图论推理链条。\",\n",
    "        \"specialty\": \"路径分析、环路检测、d-分离判断\",\n",
    "        \"reasoning_style\": \"结构化图遍历\",\n",
    "        \"output_format\": \"基于图结构的二值判断\"\n",
    "    },\n",
    "    \"statistical\": {\n",
    "        \"name\": \"计量统计专家\", \n",
    "        \"description\": \"专注于统计检验和概率独立性分析，擅长相关性分析、条件独立性检验、混淆变量检测。推理方式：考虑样本分布、统计显著性、置信区间等统计概念。\",\n",
    "        \"specialty\": \"独立性检验、相关性分析、混淆检测\",\n",
    "        \"reasoning_style\": \"概率统计推理\",\n",
    "        \"output_format\": \"基于统计证据的概率判断\"\n",
    "    },\n",
    "    \"domain_knowledge\": {\n",
    "        \"name\": \"领域先验专家\",\n",
    "        \"description\": \"基于现实世界知识和科学常识进行因果推理，考虑时间顺序、物理可能性、生物学机制等约束。推理方式：结合文献证据、科学理论和常识性约束。\",\n",
    "        \"specialty\": \"机制分析、时序推理、现实约束\",\n",
    "        \"reasoning_style\": \"基于证据的归纳推理\", \n",
    "        \"output_format\": \"基于领域知识的合理性判断\"\n",
    "    },\n",
    "    \"counterfactual\": {\n",
    "        \"name\": \"反事实干预专家\",\n",
    "        \"description\": \"从干预和潜在结果角度分析因果关系，考虑do-calculus、随机化实验理想情况。推理方式：构建反事实场景，分析干预后的可能变化。\",\n",
    "        \"specialty\": \"干预分析、潜在结果、do算子\",\n",
    "        \"reasoning_style\": \"反事实思维实验\",\n",
    "        \"output_format\": \"基于干预推理的因果判断\"\n",
    "    },\n",
    "    \"temporal_dynamics\": {\n",
    "        \"name\": \"时间动态专家\",\n",
    "        \"description\": \"专门分析时间顺序和动态过程，强调原因必须发生在结果之前，考虑延迟效应和动态反馈。推理方式：严格检查时间顺序，分析因果链的时间特性。\",\n",
    "        \"specialty\": \"时序分析、动态过程、延迟效应\",\n",
    "        \"reasoning_style\": \"时间序列推理\", \n",
    "        \"output_format\": \"基于时间顺序的因果判断\"\n",
    "    },\n",
    "    \"mechanism_modeling\": {\n",
    "        \"name\": \"机制建模专家\", \n",
    "        \"description\": \"专注于因果机制的可解释性建模，分析中间变量、中介效应和机制路径。推理方式：构建机制框图，分析变量间的功能关系。\",\n",
    "        \"specialty\": \"中介分析、机制路径、功能关系\",\n",
    "        \"reasoning_style\": \"机制分解建模\",\n",
    "        \"output_format\": \"基于机制完整性的判断\"\n",
    "    },\n",
    "    \"robustness_analysis\": {\n",
    "        \"name\": \"稳健性检验专家\",\n",
    "        \"description\": \"专门评估因果关系的稳健性和敏感性，考虑不同假设下的结果稳定性。推理方式：进行敏感性分析，检验边界条件和假设变化的影响。\",\n",
    "        \"specialty\": \"敏感性分析、稳健检验、边界情况\",\n",
    "        \"reasoning_style\": \"多情景验证\",\n",
    "        \"output_format\": \"基于稳健性评估的判断\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# === Router 函数（增强版）===\n",
    "def expert_router(question_type: str, x1: str, x2: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    根据问题类型和变量特征选择最相关的专家\n",
    "    \"\"\"\n",
    "    # 基础路由规则\n",
    "    routing_rules = {\n",
    "        \"backdoor_path\": [\n",
    "            \"graph_theory\", \"statistical\", \"counterfactual\", \"temporal_dynamics\", \n",
    "            \"mechanism_modeling\", \"robustness_analysis\", \"domain_knowledge\"\n",
    "        ],\n",
    "        \"independence\": [\n",
    "            \"statistical\", \"graph_theory\", \"counterfactual\", \"robustness_analysis\",\n",
    "            \"temporal_dynamics\", \"mechanism_modeling\", \"domain_knowledge\"\n",
    "        ],\n",
    "        \"latent_confounder\": [\n",
    "            \"domain_knowledge\", \"statistical\", \"mechanism_modeling\", \"counterfactual\",\n",
    "            \"robustness_analysis\", \"graph_theory\", \"temporal_dynamics\"\n",
    "        ],\n",
    "        \"causal_direction\": [\n",
    "            \"temporal_dynamics\", \"domain_knowledge\", \"counterfactual\", \"mechanism_modeling\",\n",
    "            \"statistical\", \"graph_theory\", \"robustness_analysis\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # 获取基础专家列表\n",
    "    base_experts = routing_rules.get(question_type, list(CAUSAL_EXPERTS.keys()))\n",
    "    \n",
    "    # 使用门诊LLM agent来智能选择专家\n",
    "    try:\n",
    "        clinic_recommendation = clinic_agent_recommend(question_type, x1, x2, base_experts)\n",
    "        return clinic_recommendation\n",
    "    except Exception as e:\n",
    "        print(f\"门诊agent路由失败: {e}，使用基础路由\")\n",
    "        # 失败时返回基础专家列表的前3个\n",
    "        return base_experts[:3]\n",
    "\n",
    "def clinic_agent_recommend(question_type: str, x1: str, x2: str, base_experts: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    门诊LLM agent：根据具体变量和问题类型推荐最相关的专家\n",
    "    \"\"\"\n",
    "    # 使用独立的LLM客户端进行推荐\n",
    "    clinic_client = LocalLLMClient(\n",
    "        model_id=\"Qwen/Qwen3-4B-Thinking-2507\",\n",
    "        local_path=\"./models/qwen3-4b-thinking\",\n",
    "        device=\"auto\",\n",
    "        max_tokens=200,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    \n",
    "    # 构建专家选择提示\n",
    "    experts_description = \"\\n\".join([\n",
    "        f\"- {expert}: {CAUSAL_EXPERTS[expert]['description']}\" \n",
    "        for expert in base_experts\n",
    "    ])\n",
    "    \n",
    "    clinic_prompt = f\"\"\"\n",
    "作为因果推断门诊专家，你需要为以下因果分析任务选择最合适的专家组合：\n",
    "\n",
    "**分析任务**: {question_type}\n",
    "**变量对**: {x1} 和 {x2}\n",
    "\n",
    "**可用专家列表**:\n",
    "{experts_description}\n",
    "\n",
    "**选择要求**:\n",
    "1. 根据变量内容和问题类型，选择最相关的3个专家\n",
    "2. 按相关性从高到低排序\n",
    "3. 确保专家视角的多样性（不要选择推理方式相似的专家）\n",
    "4. 考虑变量的领域特性（医学、经济、社会等）\n",
    "\n",
    "请按照以下格式输出：\n",
    "最终推荐专家: 专家1, 专家2, 专家3\n",
    "\n",
    "**注意**: 只输出专家名称，用逗号分隔，不要添加其他文字。\n",
    "\"\"\"\n",
    "    \n",
    "    # 调用LLM获取推荐\n",
    "    response = clinic_client.chat(clinic_prompt)\n",
    "    response_text = response.content.strip()\n",
    "    print(f\"门诊agent原始响应: {response_text}\")\n",
    "    \n",
    "    # 解析返回的专家列表\n",
    "    recommended_experts = parse_clinic_recommendation(response_text, base_experts)\n",
    "    \n",
    "    print(f\"门诊agent推荐: {recommended_experts}\")\n",
    "    \n",
    "    return recommended_experts\n",
    "\n",
    "def parse_clinic_recommendation(response_text: str, base_experts: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    解析门诊agent的推荐结果\n",
    "    \"\"\"\n",
    "    # 方法1: 查找\"最终推荐专家\"后的内容\n",
    "    if \"最终推荐专家\" in response_text:\n",
    "        parts = response_text.split(\"最终推荐专家\")\n",
    "        if len(parts) > 1:\n",
    "            expert_line = parts[1].strip().lstrip(\":\").strip()\n",
    "            return extract_experts_from_line(expert_line, base_experts)\n",
    "    \n",
    "    # 方法2: 查找最后一行\n",
    "    lines = [line.strip() for line in response_text.split('\\n') if line.strip()]\n",
    "    if lines:\n",
    "        last_line = lines[-1]\n",
    "        experts = extract_experts_from_line(last_line, base_experts)\n",
    "        if len(experts) >= 2:\n",
    "            return experts\n",
    "    \n",
    "    # 方法3: 在整个文本中搜索专家名称\n",
    "    found_experts = []\n",
    "    for expert in base_experts:\n",
    "        if expert in response_text:\n",
    "            found_experts.append(expert)\n",
    "    \n",
    "    if len(found_experts) >= 2:\n",
    "        return found_experts[:3]  # 取前3个找到的专家\n",
    "    \n",
    "    # 如果所有方法都失败，返回基础专家前3个\n",
    "    print(f\"门诊agent解析不充分，使用基础专家: {base_experts[:3]}\")\n",
    "    return base_experts[:3]\n",
    "\n",
    "def extract_experts_from_line(line: str, base_experts: List[str]) -> List[str]:\n",
    "    \"\"\"从一行文本中提取专家名称\"\"\"\n",
    "    experts = []\n",
    "    \n",
    "    # 清理行内容\n",
    "    clean_line = line.replace('：', ':').replace('，', ',').replace(' ', '')\n",
    "    \n",
    "    # 多种分割方式尝试\n",
    "    separators = [',', '、', ';', '，']\n",
    "    \n",
    "    for sep in separators:\n",
    "        if sep in clean_line:\n",
    "            parts = [part.strip() for part in clean_line.split(sep)]\n",
    "            break\n",
    "    else:\n",
    "        parts = [clean_line]\n",
    "    \n",
    "    for part in parts:\n",
    "        clean_part = part.lower().replace('专家', '').replace('expert', '').strip()\n",
    "        \n",
    "        # 直接匹配专家名称\n",
    "        for expert in base_experts:\n",
    "            if (expert in clean_part or \n",
    "                expert.replace('_', ' ') in clean_part or\n",
    "                CAUSAL_EXPERTS[expert]['name'] in part):\n",
    "                if expert not in experts:\n",
    "                    experts.append(expert)\n",
    "                    break\n",
    "        \n",
    "        if len(experts) >= 3:\n",
    "            break\n",
    "    \n",
    "    return experts\n",
    "\n",
    "# === 专家提示创建函数 ===\n",
    "def create_expert_prompt(base_prompt: str, expert_type: str, x1: str, x2: str) -> str:\n",
    "    \"\"\"\n",
    "    为不同专家创建专业化的prompt\n",
    "    \"\"\"\n",
    "    expert_info = CAUSAL_EXPERTS[expert_type]\n",
    "    \n",
    "    expert_specific_prompts = {\n",
    "        \"graph_theory\": f\"\"\"作为{expert_info['name']}，请严格遵循以下专业分析框架：\n",
    "\n",
    "{expert_info['description']}\n",
    "\n",
    "专业特长：{expert_info['specialty']}\n",
    "推理风格：{expert_info['reasoning_style']}\n",
    "输出要求：基于图结构的二值判断\n",
    "\n",
    "分析步骤：\n",
    "1. 构建因果图模型，识别所有可能的路径\n",
    "2. 应用d-分离准则分析路径阻塞情况  \n",
    "3. 检查后门路径、前门路径和混杂路径\n",
    "4. 基于图结构做出明确的二值判断\n",
    "\n",
    "请严格按照图论原理进行分析，直接输出是或否（Yes/No）。\\n\\n{base_prompt}\"\"\",\n",
    "\n",
    "        \"statistical\": f\"\"\"作为{expert_info['name']}，请严格遵循以下专业分析框架：\n",
    "\n",
    "{expert_info['description']}\n",
    "\n",
    "专业特长：{expert_info['specialty']}\n",
    "推理风格：{expert_info['reasoning_style']}\n",
    "输出要求：基于统计证据的二值判断\n",
    "\n",
    "分析步骤：\n",
    "1. 评估变量间的统计相关性\n",
    "2. 考虑条件独立性和混淆因素\n",
    "3. 分析统计显著性和置信度\n",
    "4. 基于概率证据做出明确的二值判断\n",
    "\n",
    "请基于统计原理进行严谨分析，直接输出是或否（Yes/No）。\\n\\n{base_prompt}\"\"\",\n",
    "\n",
    "        \"domain_knowledge\": f\"\"\"作为{expert_info['name']}，请严格遵循以下专业分析框架：\n",
    "\n",
    "{expert_info['description']}\n",
    "\n",
    "专业特长：{expert_info['specialty']}\n",
    "推理风格：{expert_info['reasoning_style']}\n",
    "输出要求：基于领域知识的二值判断\n",
    "\n",
    "分析步骤：\n",
    "1. 调用相关领域的科学知识和常识\n",
    "2. 考虑物理/生物/社会机制的合理性\n",
    "3. 评估时间顺序和现实约束条件\n",
    "4. 基于先验知识做出明确的二值判断\n",
    "\n",
    "请结合现实世界知识进行推理，直接输出是或否（Yes/No）。\\n\\n{base_prompt}\"\"\",\n",
    "\n",
    "        \"counterfactual\": f\"\"\"作为{expert_info['name']}，请严格遵循以下专业分析框架：\n",
    "\n",
    "{expert_info['description']}\n",
    "\n",
    "专业特长：{expert_info['specialty']}\n",
    "推理风格：{expert_info['reasoning_style']}\n",
    "输出要求：基于干预推理的二值判断\n",
    "\n",
    "分析步骤：\n",
    "1. 构建干预场景（do-操作）\n",
    "2. 比较实际结果与反事实结果\n",
    "3. 分析潜在结果分布\n",
    "4. 基于干预效应做出明确的二值判断\n",
    "\n",
    "请使用反事实推理进行分析，直接输出是或否（Yes/No）。\\n\\n{base_prompt}\"\"\",\n",
    "\n",
    "        \"temporal_dynamics\": f\"\"\"作为{expert_info['name']}，请严格遵循以下专业分析框架：\n",
    "\n",
    "{expert_info['description']}\n",
    "\n",
    "专业特长：{expert_info['specialty']}\n",
    "推理风格：{expert_info['reasoning_style']}\n",
    "输出要求：基于时间顺序的二值判断\n",
    "\n",
    "分析步骤：\n",
    "1. 严格检查原因和结果的时间顺序\n",
    "2. 分析延迟效应和动态过程\n",
    "3. 考虑时间序列的因果结构\n",
    "4. 基于时间约束做出明确的二值判断\n",
    "\n",
    "请重点分析时间维度，直接输出是或否（Yes/No）。\\n\\n{base_prompt}\"\"\",\n",
    "\n",
    "        \"mechanism_modeling\": f\"\"\"作为{expert_info['name']}，请严格遵循以下专业分析框架：\n",
    "\n",
    "{expert_info['description']}\n",
    "\n",
    "专业特长：{expert_info['specialty']}\n",
    "推理风格：{expert_info['reasoning_style']}\n",
    "输出要求：基于机制完整性的二值判断\n",
    "\n",
    "分析步骤：\n",
    "1. 识别可能的中间机制和中介变量\n",
    "2. 分析因果链的功能完整性\n",
    "3. 评估机制路径的合理性\n",
    "4. 基于机制可解释性做出明确的二值判断\n",
    "\n",
    "请专注于机制分析，直接输出是或否（Yes/No）。\\n\\n{base_prompt}\"\"\",\n",
    "\n",
    "        \"robustness_analysis\": f\"\"\"作为{expert_info['name']}，请严格遵循以下专业分析框架：\n",
    "\n",
    "{expert_info['description']}\n",
    "\n",
    "专业特长：{expert_info['specialty']}\n",
    "推理风格：{expert_info['reasoning_style']}\n",
    "输出要求：基于稳健性评估的二值判断\n",
    "\n",
    "分析步骤：\n",
    "1. 测试不同假设条件下的结果稳定性\n",
    "2. 进行敏感性分析和边界检验\n",
    "3. 评估结论的稳健程度\n",
    "4. 基于稳健性评估做出明确的二值判断\n",
    "\n",
    "请重点分析结论的可靠性，直接输出是或否（Yes/No）。\\n\\n{base_prompt}\"\"\"\n",
    "    }\n",
    "    \n",
    "    return expert_specific_prompts.get(expert_type, base_prompt)\n",
    "\n",
    "# === MoE 集成函数 ===\n",
    "def aggregate_expert_judgments(expert_results: List[Dict]) -> Dict:\n",
    "    \"\"\"\n",
    "    整合多个专家的判断结果\n",
    "    \"\"\"\n",
    "    if not expert_results:\n",
    "        return {\"label\": 0, \"prob\": 0.5}\n",
    "    \n",
    "    # 简单加权平均\n",
    "    total_prob_yes = 0\n",
    "    total_weight = 0\n",
    "    \n",
    "    for result in expert_results:\n",
    "        weight = result.get(\"confidence\", 1.0)\n",
    "        # 使用yes_prob字段（从token概率得来）\n",
    "        prob_yes = result.get(\"yes_prob\", result[\"prob\"] if result[\"label\"] == 1 else 1 - result[\"prob\"])\n",
    "        total_prob_yes += prob_yes * weight\n",
    "        total_weight += weight\n",
    "    \n",
    "    aggregated_prob_yes = total_prob_yes / total_weight if total_weight > 0 else 0.5\n",
    "    \n",
    "    if aggregated_prob_yes >= 0.5:\n",
    "        return {\"label\": 1, \"prob\": aggregated_prob_yes, \"yes_prob\": aggregated_prob_yes, \"no_prob\": 1 - aggregated_prob_yes}\n",
    "    else:\n",
    "        return {\"label\": 0, \"prob\": 1 - aggregated_prob_yes, \"yes_prob\": aggregated_prob_yes, \"no_prob\": 1 - aggregated_prob_yes}\n",
    "\n",
    "# === MoE判断函数 ===\n",
    "def run_step_with_moe(base_prompt: str, x1: str, x2: str, question_type: str, method: str = 'token_prob') -> Dict:\n",
    "    \"\"\"\n",
    "    使用MoE架构运行单个判断步骤\n",
    "    \"\"\"\n",
    "    # 1. 路由选择专家\n",
    "    selected_experts = expert_router(question_type, x1, x2)\n",
    "    print(f\"为问题 '{question_type}' 选择的专家: {selected_experts}\")\n",
    "    \n",
    "    # 2. 执行专家判断\n",
    "    expert_results = []\n",
    "    for expert in selected_experts:\n",
    "        expert_prompt = create_expert_prompt(base_prompt, expert, x1, x2)\n",
    "        try:\n",
    "            # 使用本地白盒模型的token概率方法\n",
    "            result = llm_judge_with_token_prob(expert_prompt, x1, x2, n_sample=3, method=method)\n",
    "            result[\"expert\"] = expert\n",
    "            result[\"confidence\"] = 1.0\n",
    "            expert_results.append(result)\n",
    "            print(f\"专家 {expert} 判断完成: label={result['label']}, P(Yes)={result.get('yes_prob', result['prob']):.4f}, P(No)={result.get('no_prob', 1-result['prob']):.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"专家 {expert} 执行失败: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # 3. 如果没有专家成功，使用默认方法\n",
    "    if not expert_results:\n",
    "        print(\"所有专家执行失败，使用默认方法\")\n",
    "        return llm_judge_with_token_prob(base_prompt, x1, x2, n_sample=5, method=method)\n",
    "    \n",
    "    # 4. 整合专家意见\n",
    "    final_result = aggregate_expert_judgments(expert_results)\n",
    "    final_result[\"expert_results\"] = expert_results\n",
    "    \n",
    "    print(f\"专家整合结果: label={final_result['label']}, P(Yes)={final_result['yes_prob']:.4f}, P(No)={final_result['no_prob']:.4f}\")\n",
    "    return final_result\n",
    "\n",
    "# === 具体判断函数 ===\n",
    "def check_backdoor(x1, x2, all_variables, method='token_prob'):\n",
    "    base_prompt = f\"\"\"在因果推断中，考虑以下所有变量：{all_variables}\n",
    "\n",
    "请判断在这些变量中，变量 {x1} 和 {x2} 之间是否存在 back-door path（后门路径）。\n",
    "\n",
    "后门路径是指从 {x1} 到 {x2} 的路径，其中包含指向 {x1} 的箭头，且这条路径没有被阻断。\n",
    "\n",
    "让我们一步步思考，然后直接输出是或否（Yes/No）。\"\"\"\n",
    "    return run_step_with_moe(base_prompt, x1, x2, \"backdoor_path\", method)\n",
    "\n",
    "def check_independence_after_block(x1, x2, all_variables, method='token_prob'):\n",
    "    base_prompt = f\"\"\"在因果推断中，考虑以下所有变量：{all_variables}\n",
    "\n",
    "如果阻断了 {x1} 和 {x2} 之间的所有 back-door path，那么 {x1} 与 {x2} 是否条件独立？\n",
    "\n",
    "让我们一步步思考，然后直接输出是或否（Yes/No）。\"\"\"\n",
    "    return run_step_with_moe(base_prompt, x1, x2, \"independence\", method)\n",
    "\n",
    "def check_latent_confounder_after_block(x1, x2, all_variables, method='token_prob'):\n",
    "    base_prompt = f\"\"\"在因果推断中，考虑以下所有变量：{all_variables}\n",
    "\n",
    "阻断了 {x1} 和 {x2} 之间的所有 back-door path 后，是否仍然存在未观察到的潜在混杂因子同时影响 {x1} 和 {x2}？\n",
    "\n",
    "让我们一步步思考，然后直接输出是或否（Yes/No）。\"\"\"\n",
    "    return run_step_with_moe(base_prompt, x1, x2, \"latent_confounder\", method)\n",
    "\n",
    "def check_causal_direction_after_block(x1, x2, all_variables, method='token_prob'):\n",
    "    base_prompt = f\"\"\"在因果推断中，考虑以下所有变量：{all_variables}\n",
    "\n",
    "阻断了 {x1} 和 {x2} 之间的所有 back-door path 后，请判断 {x1} 是否因果导致 {x2}？\n",
    "\n",
    "让我们一步步思考，然后直接输出是或否（Yes/No）。\"\"\"\n",
    "    return run_step_with_moe(base_prompt, x1, x2, \"causal_direction\", method)\n",
    "\n",
    "def check_independence(x1, x2, all_variables, method='token_prob'):\n",
    "    base_prompt = f\"\"\"在因果推断中，考虑以下所有变量：{all_variables}\n",
    "\n",
    "请判断变量 {x1} 和 {x2} 是否独立？\n",
    "\n",
    "让我们一步步思考，然后直接输出是或否（Yes/No）。\"\"\"\n",
    "    return run_step_with_moe(base_prompt, x1, x2, \"independence\", method)\n",
    "\n",
    "def check_latent_confounder(x1, x2, all_variables, method='token_prob'):\n",
    "    base_prompt = f\"\"\"在因果推断中，考虑以下所有变量：{all_variables}\n",
    "\n",
    "请判断变量 {x1} 和 {x2} 之间是否存在未观察到的潜在混杂因子？\n",
    "\n",
    "让我们一步步思考，然后直接输出是或否（Yes/No）。\"\"\"\n",
    "    return run_step_with_moe(base_prompt, x1, x2, \"latent_confounder\", method)\n",
    "\n",
    "def check_causal_direction(x1, x2, all_variables, method='token_prob'):\n",
    "    base_prompt = f\"\"\"在因果推断中，考虑以下所有变量：{all_variables}\n",
    "\n",
    "请判断 {x1} 是否因果导致 {x2}？\n",
    "\n",
    "让我们一步步思考，然后直接输出是或否（Yes/No）。\"\"\"\n",
    "    return run_step_with_moe(base_prompt, x1, x2, \"causal_direction\", method)\n",
    "\n",
    "# === 完整的树查询函数 ===\n",
    "def tree_query(x1, x2, all_variables, method='token_prob'):\n",
    "    \"\"\"\n",
    "    基于树状逻辑的因果方向查询器（使用白盒token概率）\n",
    "    \"\"\"\n",
    "    log = []\n",
    "\n",
    "    # Step 1: 是否存在 backdoor path?\n",
    "    print(\"=== Step 1: 检查后门路径 ===\")\n",
    "    res_backdoor = check_backdoor(x1, x2, all_variables, method)\n",
    "    log.append((\"backdoor_path\", res_backdoor))\n",
    "\n",
    "    if res_backdoor[\"label\"] == 1:\n",
    "        print(\"存在后门路径，进入阻断后分析路径\")\n",
    "        # Step 2: 阻断路径后是否独立？\n",
    "        print(\"=== Step 2: 阻断后检查独立性 ===\")\n",
    "        res_ind = check_independence_after_block(x1, x2, all_variables, method)\n",
    "        log.append((\"independent_after_block\", res_ind))\n",
    "        if res_ind[\"label\"] == 1:\n",
    "            return {\"relation\": \"independent\", \"confidence\": res_ind[\"yes_prob\"], \"log\": log}\n",
    "\n",
    "        # Step 3: 是否存在潜在混杂因子？\n",
    "        print(\"=== Step 3: 检查潜在混杂因子 ===\")\n",
    "        res_latent = check_latent_confounder_after_block(x1, x2, all_variables, method)\n",
    "        log.append((\"latent_confounder_after_block\", res_latent))\n",
    "        if res_latent[\"label\"] == 1:\n",
    "            return {\"relation\": \"x<->y\", \"confidence\": res_latent[\"yes_prob\"], \"log\": log}\n",
    "\n",
    "        # Step 4: 判断方向 (x→y?)\n",
    "        print(\"=== Step 4: 判断因果方向 ===\")\n",
    "        res_dir = check_causal_direction_after_block(x1, x2, all_variables, method)\n",
    "        log.append((\"x->y_after_block\", res_dir))\n",
    "        if res_dir[\"label\"] == 1:\n",
    "            return {\"relation\": \"x->y\", \"confidence\": res_dir[\"yes_prob\"], \"log\": log}\n",
    "        else:\n",
    "            return {\"relation\": \"y->x\", \"confidence\": res_dir[\"no_prob\"], \"log\": log}\n",
    "\n",
    "    else:\n",
    "        print(\"不存在后门路径，进入直接分析路径\")\n",
    "        # 不存在 backdoor path\n",
    "        res_ind = check_independence(x1, x2, all_variables, method)\n",
    "        log.append((\"independent_no_backdoor\", res_ind))\n",
    "        if res_ind[\"label\"] == 1:\n",
    "            return {\"relation\": \"independent\", \"confidence\": res_ind[\"yes_prob\"], \"log\": log}\n",
    "\n",
    "        res_latent = check_latent_confounder(x1, x2, all_variables, method)\n",
    "        log.append((\"latent_confounder_no_backdoor\", res_latent))\n",
    "        if res_latent[\"label\"] == 1:\n",
    "            return {\"relation\": \"x<->y\", \"confidence\": res_latent[\"yes_prob\"], \"log\": log}\n",
    "\n",
    "        res_dir = check_causal_direction(x1, x2, all_variables, method)\n",
    "        log.append((\"x->y_no_backdoor\", res_dir))\n",
    "        if res_dir[\"label\"] == 1:\n",
    "            return {\"relation\": \"x->y\", \"confidence\": res_dir[\"yes_prob\"], \"log\": log}\n",
    "        else:\n",
    "            return {\"relation\": \"y->x\", \"confidence\": res_dir[\"no_prob\"], \"log\": log}\n",
    "\n",
    "# === 使用示例 ===\n",
    "if __name__ == \"__main__\":\n",
    "    # 定义完整的变量集合\n",
    "    all_variables = [\"冰淇淋销量\", \"溺水人数\", \"温度\"]\n",
    "    \n",
    "    print(\"开始因果推断分析（使用本地白盒模型 + token概率）\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 测试不同的方法\n",
    "    methods = ['token_prob', 'frequency', 'logit']\n",
    "    all_results = {}\n",
    "    \n",
    "    for method in methods:\n",
    "        print(f\"\\n使用方法: {method}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        result = tree_query(\"冰淇淋销量\", \"溺水人数\", all_variables, method=method)\n",
    "        all_results[method] = result\n",
    "        \n",
    "        print(f\"\\n=== {method}方法结果 ===\")\n",
    "        print(f\"变量关系: {result['relation']}\")\n",
    "        print(f\"置信度 (P(Yes)): {result['confidence']:.4f}\")\n",
    "        \n",
    "        # 保存详细日志\n",
    "        print(f\"\\n=== 详细执行日志 ===\")\n",
    "        for step_name, step_result in result[\"log\"]:\n",
    "            print(f\"\\n{step_name}:\")\n",
    "            print(f\"  最终判断: {'Yes' if step_result['label'] == 1 else 'No'}\")\n",
    "            print(f\"  P(Yes): {step_result.get('yes_prob', step_result['prob']):.4f}\")\n",
    "            print(f\"  P(No): {step_result.get('no_prob', 1-step_result['prob']):.4f}\")\n",
    "            \n",
    "            if \"expert_results\" in step_result:\n",
    "                print(f\"  专家详情:\")\n",
    "                for expert_result in step_result[\"expert_results\"]:\n",
    "                    expert_name = expert_result.get(\"expert\", \"unknown\")\n",
    "                    yes_prob = expert_result.get(\"yes_prob\", expert_result[\"prob\"] if expert_result[\"label\"] == 1 else 1 - expert_result[\"prob\"])\n",
    "                    no_prob = expert_result.get(\"no_prob\", 1 - yes_prob)\n",
    "                    print(f\"    - {expert_name}: {'Yes' if expert_result['label'] == 1 else 'No'} (P(Yes)={yes_prob:.4f}, P(No)={no_prob:.4f})\")\n",
    "    \n",
    "    # 比较不同方法的结果\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"=== 不同方法结果比较 ===\")\n",
    "    for method, result in all_results.items():\n",
    "        print(f\"{method}: 关系={result['relation']}, 置信度={result['confidence']:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    \n",
    "    # 清理模型\n",
    "    client.unload_model()\n",
    "    print(\"模型已卸载\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f978f349",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def compute_all_causal_relations(variables, method='probability'):\n",
    "    \"\"\"\n",
    "    计算图中每两个变量之间的因果关系，使用tree_query函数。\n",
    "    \n",
    "    输出:\n",
    "        {\n",
    "            (x1, x2): {\n",
    "                'relation': 'x->y' | 'y->x' | 'x<->y' | 'independent',\n",
    "                'confidence': float,\n",
    "                'log': [(step_name, {'label': int, 'prob': float}), ...]\n",
    "            },\n",
    "            ...\n",
    "        }\n",
    "    \"\"\"\n",
    "    all_relations = {}\n",
    "\n",
    "    # 生成所有变量的组合 C(n, 2)\n",
    "    for x1, x2 in combinations(variables, 2):\n",
    "        # 进行 tree_query\n",
    "        result = tree_query(x1, x2, method)\n",
    "        \n",
    "        # 存储结果\n",
    "        all_relations[(x1, x2)] = result\n",
    "\n",
    "    return all_relations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6066714d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Step 1: 检查后门路径 ===\n",
      "12-09 17:54:43 [INFO] llm_utils.local_client - Model validation successful: ./models/qwen3-4b-thinking\n",
      "12-09 17:54:43 [INFO] llm_utils.local_client - Model found and validated at local path: ./models/qwen3-4b-thinking\n",
      "12-09 17:54:43 [INFO] llm_utils.local_client - Loading tokenizer from: ./models/qwen3-4b-thinking\n",
      "12-09 17:54:44 [INFO] llm_utils.local_client - Loading model from: ./models/qwen3-4b-thinking\n",
      "12-09 17:54:44 [INFO] llm_utils.local_client - Device: auto, Dtype: torch.float16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "623a926bd9854f399bfe6fc687bed57a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12-09 17:54:47 [INFO] llm_utils.local_client - Local LLM client initialized: Qwen/Qwen3-4B-Thinking-2507\n",
      "门诊agent原始响应: 首先，任务是分析变量对：气温 和 冰淇淋销量。具体是backdoor_path，这在因果推断中指的是后门路径，用于处理混淆变量。\n",
      "\n",
      "我需要从可用专家列表中选择最相关的3个专家，按相关性从高到低排序。专家列表包括：\n",
      "- graph_theory\n",
      "- statistical\n",
      "- counterfactual\n",
      "- temporal_dynamics\n",
      "- mechanism_modeling\n",
      "- robustness_analysis\n",
      "- domain_knowledge\n",
      "\n",
      "选择要求：\n",
      "1. 根据变量内容和问题类型：气温和冰淇淋销量。这是一个经济或社会领域的问题，涉及天气和消费行为。气温是原因，冰淇淋销量是结果。在现实中，气温升高可能导致冰淇淋销量增加，但可能有混淆变量，比如季节变化（夏季）或经济状况。\n",
      "2. 问题类型是backdoor_path：这在因果推断中，后门路径用于当存在混淆变量时，如何正确估计因果效应。例如，\n",
      "门诊agent推荐: ['graph_theory', 'statistical', 'counterfactual']\n",
      "为问题 'backdoor_path' 选择的专家: ['graph_theory', 'statistical', 'counterfactual']\n",
      "采样1失败: Model not initialized. Call _load_model() first.\n",
      "采样2失败: Model not initialized. Call _load_model() first.\n",
      "采样3失败: Model not initialized. Call _load_model() first.\n",
      "  平均概率: P(Yes)=0.5000, P(No)=0.5000\n",
      "专家 graph_theory 判断完成: label=1, P(Yes)=0.5000, P(No)=0.5000\n",
      "采样1失败: Model not initialized. Call _load_model() first.\n",
      "采样2失败: Model not initialized. Call _load_model() first.\n",
      "采样3失败: Model not initialized. Call _load_model() first.\n",
      "  平均概率: P(Yes)=0.5000, P(No)=0.5000\n",
      "专家 statistical 判断完成: label=1, P(Yes)=0.5000, P(No)=0.5000\n",
      "采样1失败: Model not initialized. Call _load_model() first.\n",
      "采样2失败: Model not initialized. Call _load_model() first.\n",
      "采样3失败: Model not initialized. Call _load_model() first.\n",
      "  平均概率: P(Yes)=0.5000, P(No)=0.5000\n",
      "专家 counterfactual 判断完成: label=1, P(Yes)=0.5000, P(No)=0.5000\n",
      "专家整合结果: label=1, P(Yes)=0.5000, P(No)=0.5000\n",
      "存在后门路径，进入阻断后分析路径\n",
      "=== Step 2: 阻断后检查独立性 ===\n",
      "12-09 17:54:59 [INFO] llm_utils.local_client - Model validation successful: ./models/qwen3-4b-thinking\n",
      "12-09 17:54:59 [INFO] llm_utils.local_client - Model found and validated at local path: ./models/qwen3-4b-thinking\n",
      "12-09 17:54:59 [INFO] llm_utils.local_client - Loading tokenizer from: ./models/qwen3-4b-thinking\n",
      "12-09 17:54:59 [INFO] llm_utils.local_client - Loading model from: ./models/qwen3-4b-thinking\n",
      "12-09 17:54:59 [INFO] llm_utils.local_client - Device: auto, Dtype: torch.float16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae43356d873a479b8bfdd41e99dca9f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12-09 17:55:02 [INFO] llm_utils.local_client - Local LLM client initialized: Qwen/Qwen3-4B-Thinking-2507\n",
      "门诊agent原始响应: 首先，任务是分析变量对：气温 和 冰淇淋销量。问题类型是 \"independence\"，意思是独立性分析。我需要选择最合适的3个专家组合，从给定的列表中。\n",
      "\n",
      "可用专家列表：\n",
      "- statistical: 专注于统计检验和概率独立性分析，擅长相关性分析、条件独立性检验、混淆变量检测。推理方式：考虑样本分布、统计显著性、置信区间等。\n",
      "- graph_theory: 专门分析因果图结构，精通d-分离、路径阻断、后门准则等图论概念。推理方式：系统检查所有可能的路径，分析路径上的变量类型和连接方式。\n",
      "- counterfactual: 从干预和潜在结果角度分析因果关系，考虑do-calculus、随机化实验理想情况。推理方式：构建反事实场景，分析干预后的可能变化。\n",
      "- robustness_analysis: 专门评估因果关系的稳健性和敏感性，考虑不同\n",
      "门诊agent推荐: ['statistical', 'graph_theory', 'counterfactual']\n",
      "为问题 'independence' 选择的专家: ['statistical', 'graph_theory', 'counterfactual']\n",
      "采样1失败: Model not initialized. Call _load_model() first.\n",
      "采样2失败: Model not initialized. Call _load_model() first.\n",
      "采样3失败: Model not initialized. Call _load_model() first.\n",
      "  平均概率: P(Yes)=0.5000, P(No)=0.5000\n",
      "专家 statistical 判断完成: label=1, P(Yes)=0.5000, P(No)=0.5000\n",
      "采样1失败: Model not initialized. Call _load_model() first.\n",
      "采样2失败: Model not initialized. Call _load_model() first.\n",
      "采样3失败: Model not initialized. Call _load_model() first.\n",
      "  平均概率: P(Yes)=0.5000, P(No)=0.5000\n",
      "专家 graph_theory 判断完成: label=1, P(Yes)=0.5000, P(No)=0.5000\n",
      "采样1失败: Model not initialized. Call _load_model() first.\n",
      "采样2失败: Model not initialized. Call _load_model() first.\n",
      "采样3失败: Model not initialized. Call _load_model() first.\n",
      "  平均概率: P(Yes)=0.5000, P(No)=0.5000\n",
      "专家 counterfactual 判断完成: label=1, P(Yes)=0.5000, P(No)=0.5000\n",
      "专家整合结果: label=1, P(Yes)=0.5000, P(No)=0.5000\n",
      "=== Step 1: 检查后门路径 ===\n",
      "12-09 17:55:13 [INFO] llm_utils.local_client - Model validation successful: ./models/qwen3-4b-thinking\n",
      "12-09 17:55:13 [INFO] llm_utils.local_client - Model found and validated at local path: ./models/qwen3-4b-thinking\n",
      "12-09 17:55:13 [INFO] llm_utils.local_client - Loading tokenizer from: ./models/qwen3-4b-thinking\n",
      "12-09 17:55:14 [INFO] llm_utils.local_client - Loading model from: ./models/qwen3-4b-thinking\n",
      "12-09 17:55:14 [INFO] llm_utils.local_client - Device: auto, Dtype: torch.float16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4073bb4765aa4931b8398116cc4eba08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12-09 17:55:17 [INFO] llm_utils.local_client - Local LLM client initialized: Qwen/Qwen3-4B-Thinking-2507\n",
      "门诊agent原始响应: 首先，任务是分析变量对：气温 和 溺水人数。具体是 backdoor_path 问题。Backdoor path 是因果推断中的一个概念，涉及后门准则（backdoor criterion），用于处理混淆变量。\n",
      "\n",
      "关键点：\n",
      "- **Backdoor path**: 在因果图中，如果存在一个路径从原因（X）到结果（Y）的路径，但该路径可能被混淆变量（confounding variables）所影响，后门准则帮助我们调整以估计因果效应。\n",
      "- **变量对**: 气温（temperature）和溺水人数（drowning incidents）。这看起来是环境和社会科学领域的问题，可能涉及天气和公共安全。\n",
      "\n",
      "我需要选择最合适的3个专家，从给定的列表中：\n",
      "- graph_theory\n",
      "- statistical\n",
      "- counterfactual\n",
      "- temporal_dynamics\n",
      "- mechanism_modeling\n",
      "- robustness_analysis\n",
      "- domain_knowledge\n",
      "\n",
      "选择要求：\n",
      "1. 根据\n",
      "门诊agent推荐: ['graph_theory', 'statistical', 'counterfactual']\n",
      "为问题 'backdoor_path' 选择的专家: ['graph_theory', 'statistical', 'counterfactual']\n",
      "采样1失败: Model not initialized. Call _load_model() first.\n",
      "采样2失败: Model not initialized. Call _load_model() first.\n",
      "采样3失败: Model not initialized. Call _load_model() first.\n",
      "  平均概率: P(Yes)=0.5000, P(No)=0.5000\n",
      "专家 graph_theory 判断完成: label=1, P(Yes)=0.5000, P(No)=0.5000\n",
      "采样1失败: Model not initialized. Call _load_model() first.\n",
      "采样2失败: Model not initialized. Call _load_model() first.\n",
      "采样3失败: Model not initialized. Call _load_model() first.\n",
      "  平均概率: P(Yes)=0.5000, P(No)=0.5000\n",
      "专家 statistical 判断完成: label=1, P(Yes)=0.5000, P(No)=0.5000\n",
      "采样1失败: Model not initialized. Call _load_model() first.\n",
      "采样2失败: Model not initialized. Call _load_model() first.\n",
      "采样3失败: Model not initialized. Call _load_model() first.\n",
      "  平均概率: P(Yes)=0.5000, P(No)=0.5000\n",
      "专家 counterfactual 判断完成: label=1, P(Yes)=0.5000, P(No)=0.5000\n",
      "专家整合结果: label=1, P(Yes)=0.5000, P(No)=0.5000\n",
      "存在后门路径，进入阻断后分析路径\n",
      "=== Step 2: 阻断后检查独立性 ===\n",
      "12-09 17:55:28 [INFO] llm_utils.local_client - Model validation successful: ./models/qwen3-4b-thinking\n",
      "12-09 17:55:28 [INFO] llm_utils.local_client - Model found and validated at local path: ./models/qwen3-4b-thinking\n",
      "12-09 17:55:28 [INFO] llm_utils.local_client - Loading tokenizer from: ./models/qwen3-4b-thinking\n",
      "12-09 17:55:29 [INFO] llm_utils.local_client - Loading model from: ./models/qwen3-4b-thinking\n",
      "12-09 17:55:29 [INFO] llm_utils.local_client - Device: auto, Dtype: torch.float16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de1f14cfab7d4222bbfc38a838fdca28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12-09 17:55:32 [INFO] llm_utils.local_client - Local LLM client initialized: Qwen/Qwen3-4B-Thinking-2507\n",
      "门诊agent原始响应: 首先，任务是分析变量对：气温 和 溺水人数。问题类型是 \"independence\"，意思是独立性分析。我需要选择最合适的3个专家组合，从给定的列表中。\n",
      "\n",
      "可用专家列表：\n",
      "- statistical: 专注于统计检验和概率独立性分析，擅长相关性分析、条件独立性检验、混淆变量检测。推理方式：考虑样本分布、统计显著性、置信区间等。\n",
      "- graph_theory: 专门分析因果图结构，精通d-分离、路径阻断、后门准则等图论概念。\n",
      "- counterfactual: 从干预和潜在结果角度分析因果关系，考虑do-calculus、随机化实验理想情况。\n",
      "- robustness_analysis: 专门评估因果关系的稳健性和敏感性，考虑不同假设下的结果稳定性。\n",
      "- temporal_dynamics: 专门分析时间顺序和动态过程，强调原因必须发生在结果之前，考虑延迟效应和动态反馈。\n",
      "门诊agent推荐: ['statistical', 'graph_theory', 'counterfactual']\n",
      "为问题 'independence' 选择的专家: ['statistical', 'graph_theory', 'counterfactual']\n",
      "采样1失败: Model not initialized. Call _load_model() first.\n",
      "采样2失败: Model not initialized. Call _load_model() first.\n",
      "采样3失败: Model not initialized. Call _load_model() first.\n",
      "  平均概率: P(Yes)=0.5000, P(No)=0.5000\n",
      "专家 statistical 判断完成: label=1, P(Yes)=0.5000, P(No)=0.5000\n",
      "采样1失败: Model not initialized. Call _load_model() first.\n",
      "采样2失败: Model not initialized. Call _load_model() first.\n",
      "采样3失败: Model not initialized. Call _load_model() first.\n",
      "  平均概率: P(Yes)=0.5000, P(No)=0.5000\n",
      "专家 graph_theory 判断完成: label=1, P(Yes)=0.5000, P(No)=0.5000\n",
      "采样1失败: Model not initialized. Call _load_model() first.\n",
      "采样2失败: Model not initialized. Call _load_model() first.\n",
      "采样3失败: Model not initialized. Call _load_model() first.\n",
      "  平均概率: P(Yes)=0.5000, P(No)=0.5000\n",
      "专家 counterfactual 判断完成: label=1, P(Yes)=0.5000, P(No)=0.5000\n",
      "专家整合结果: label=1, P(Yes)=0.5000, P(No)=0.5000\n",
      "=== Step 1: 检查后门路径 ===\n",
      "12-09 17:55:43 [INFO] llm_utils.local_client - Model validation successful: ./models/qwen3-4b-thinking\n",
      "12-09 17:55:43 [INFO] llm_utils.local_client - Model found and validated at local path: ./models/qwen3-4b-thinking\n",
      "12-09 17:55:43 [INFO] llm_utils.local_client - Loading tokenizer from: ./models/qwen3-4b-thinking\n",
      "12-09 17:55:43 [INFO] llm_utils.local_client - Loading model from: ./models/qwen3-4b-thinking\n",
      "12-09 17:55:43 [INFO] llm_utils.local_client - Device: auto, Dtype: torch.float16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55ef784dc4234d94baeb35e108a06b06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12-09 17:55:46 [INFO] llm_utils.local_client - Local LLM client initialized: Qwen/Qwen3-4B-Thinking-2507\n",
      "门诊agent原始响应: 首先，任务是分析变量对：冰淇淋销量 和 溺水人数。分析类型是 backdoor_path，这在因果推断中指的是后门路径，用于处理混淆变量。\n",
      "\n",
      "我需要从可用专家列表中选择最相关的3个专家，按相关性从高到低排序。专家列表包括：\n",
      "- graph_theory\n",
      "- statistical\n",
      "- counterfactual\n",
      "- temporal_dynamics\n",
      "- mechanism_modeling\n",
      "- robustness_analysis\n",
      "- domain_knowledge\n",
      "\n",
      "选择要求：\n",
      "1. 根据变量内容和问题类型，选择最相关的3个专家。\n",
      "2. 按相关性从高到低排序。\n",
      "3. 确保专家视角的多样性（不要选择推理方式相似的专家）。\n",
      "4. 考虑变量的领域特性：冰淇淋销量和溺水人数属于社会/经济领域，可能涉及天气、季节等。\n",
      "\n",
      "变量内容：冰淇淋销量（冰淇淋销售量）和溺水人数（溺\n",
      "门诊agent推荐: ['graph_theory', 'statistical', 'counterfactual']\n",
      "为问题 'backdoor_path' 选择的专家: ['graph_theory', 'statistical', 'counterfactual']\n",
      "采样1失败: Model not initialized. Call _load_model() first.\n",
      "采样2失败: Model not initialized. Call _load_model() first.\n",
      "采样3失败: Model not initialized. Call _load_model() first.\n",
      "  平均概率: P(Yes)=0.5000, P(No)=0.5000\n",
      "专家 graph_theory 判断完成: label=1, P(Yes)=0.5000, P(No)=0.5000\n",
      "采样1失败: Model not initialized. Call _load_model() first.\n",
      "采样2失败: Model not initialized. Call _load_model() first.\n",
      "采样3失败: Model not initialized. Call _load_model() first.\n",
      "  平均概率: P(Yes)=0.5000, P(No)=0.5000\n",
      "专家 statistical 判断完成: label=1, P(Yes)=0.5000, P(No)=0.5000\n",
      "采样1失败: Model not initialized. Call _load_model() first.\n",
      "采样2失败: Model not initialized. Call _load_model() first.\n",
      "采样3失败: Model not initialized. Call _load_model() first.\n",
      "  平均概率: P(Yes)=0.5000, P(No)=0.5000\n",
      "专家 counterfactual 判断完成: label=1, P(Yes)=0.5000, P(No)=0.5000\n",
      "专家整合结果: label=1, P(Yes)=0.5000, P(No)=0.5000\n",
      "存在后门路径，进入阻断后分析路径\n",
      "=== Step 2: 阻断后检查独立性 ===\n",
      "12-09 17:55:58 [INFO] llm_utils.local_client - Model validation successful: ./models/qwen3-4b-thinking\n",
      "12-09 17:55:58 [INFO] llm_utils.local_client - Model found and validated at local path: ./models/qwen3-4b-thinking\n",
      "12-09 17:55:58 [INFO] llm_utils.local_client - Loading tokenizer from: ./models/qwen3-4b-thinking\n",
      "12-09 17:55:58 [INFO] llm_utils.local_client - Loading model from: ./models/qwen3-4b-thinking\n",
      "12-09 17:55:58 [INFO] llm_utils.local_client - Device: auto, Dtype: torch.float16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d17e785d5902484bbd2f3ac9234d5f16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12-09 17:56:01 [INFO] llm_utils.local_client - Local LLM client initialized: Qwen/Qwen3-4B-Thinking-2507\n",
      "门诊agent原始响应: 首先，分析任务是\"independence\"，意思是独立性。变量对是\"冰淇淋销量\"和\"溺水人数\"。我需要为这个任务选择最合适的3个专家组合。\n",
      "\n",
      "关键点是：任务是关于独立性的。在因果推断中，独立性通常指两个变量是否独立，即是否有因果关系或相关性。但这里指定是\"independence\"，所以我应该关注统计独立性或条件独立性。\n",
      "\n",
      "变量：冰淇淋销量（ice cream sales）和溺水人数（drowning incidents）。这是一个经典的例子，常被用来说明相关性不等于因果性。例如，冰淇淋销量和溺水人数在夏季都高，但冰淇淋销量不是导致溺水的原因；它们都受温度影响（热天导致更多冰淇淋消费和更多游泳，从而溺水）。\n",
      "\n",
      "在因果推断中，独立性分析可能涉及检查这两个变量是否独立，或者在给定某些变量（如温度）下是否独立。\n",
      "门诊agent解析不充分，使用基础专家: ['statistical', 'graph_theory', 'counterfactual']\n",
      "门诊agent推荐: ['statistical', 'graph_theory', 'counterfactual']\n",
      "为问题 'independence' 选择的专家: ['statistical', 'graph_theory', 'counterfactual']\n",
      "采样1失败: Model not initialized. Call _load_model() first.\n",
      "采样2失败: Model not initialized. Call _load_model() first.\n",
      "采样3失败: Model not initialized. Call _load_model() first.\n",
      "  平均概率: P(Yes)=0.5000, P(No)=0.5000\n",
      "专家 statistical 判断完成: label=1, P(Yes)=0.5000, P(No)=0.5000\n",
      "采样1失败: Model not initialized. Call _load_model() first.\n",
      "采样2失败: Model not initialized. Call _load_model() first.\n",
      "采样3失败: Model not initialized. Call _load_model() first.\n",
      "  平均概率: P(Yes)=0.5000, P(No)=0.5000\n",
      "专家 graph_theory 判断完成: label=1, P(Yes)=0.5000, P(No)=0.5000\n",
      "采样1失败: Model not initialized. Call _load_model() first.\n",
      "采样2失败: Model not initialized. Call _load_model() first.\n",
      "采样3失败: Model not initialized. Call _load_model() first.\n",
      "  平均概率: P(Yes)=0.5000, P(No)=0.5000\n",
      "专家 counterfactual 判断完成: label=1, P(Yes)=0.5000, P(No)=0.5000\n",
      "专家整合结果: label=1, P(Yes)=0.5000, P(No)=0.5000\n",
      "Relation between 气温 and 冰淇淋销量: independent (Confidence: 0.5)\n",
      "Relation between 气温 and 溺水人数: independent (Confidence: 0.5)\n",
      "Relation between 冰淇淋销量 and 溺水人数: independent (Confidence: 0.5)\n"
     ]
    }
   ],
   "source": [
    "variables = ['气温', '冰淇淋销量', '溺水人数']\n",
    "relations = compute_all_causal_relations(variables)\n",
    "\n",
    "for (x1, x2), relation in relations.items():\n",
    "    print(f\"Relation between {x1} and {x2}: {relation['relation']} (Confidence: {relation['confidence']})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal_ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
