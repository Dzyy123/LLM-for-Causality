{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff51548",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\25877\\WORKS\\LLM-for-Causality\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12-12 14:22:33 [INFO] llm_utils.local_client - Model not found locally or invalid, downloading: Qwen/Qwen3-4B-Thinking-2507\n",
      "12-12 14:22:33 [INFO] llm_utils.local_client - Downloading model from https://huggingface.co: Qwen/Qwen3-4B-Thinking-2507\n",
      "12-12 14:22:33 [INFO] llm_utils.local_client - Target directory: ./models/qwen3-4b-thinking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\25877\\WORKS\\LLM-for-Causality\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "d:\\25877\\WORKS\\LLM-for-Causality\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:979: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n",
      "Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]'(ProtocolError('Connection aborted.', ConnectionAbortedError(10053, '你的主机中的软件中止了一个已建立的连接。', None, 10053, None)), '(Request ID: e61b8bf5-9319-45ea-bdaf-f870271c6018)')' thrown while requesting HEAD https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507/resolve/768f209d9ea81521153ed38c47d515654e938aea/model-00001-of-00003.safetensors\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionAbortedError(10053, '你的主机中的软件中止了一个已建立的连接。', None, 10053, None)), '(Request ID: 8341326e-fe90-45d7-a941-716aadbc98c3)')' thrown while requesting HEAD https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507/resolve/768f209d9ea81521153ed38c47d515654e938aea/README.md\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionAbortedError(10053, '你的主机中的软件中止了一个已建立的连接。', None, 10053, None)), '(Request ID: 55473938-9960-4264-a8f0-8b3b98ccc78c)')' thrown while requesting HEAD https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507/resolve/768f209d9ea81521153ed38c47d515654e938aea/merges.txt\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionAbortedError(10053, '你的主机中的软件中止了一个已建立的连接。', None, 10053, None)), '(Request ID: afa5aacd-7b5b-49e2-91d9-ed1dfe37fb25)')' thrown while requesting HEAD https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507/resolve/768f209d9ea81521153ed38c47d515654e938aea/README.md\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionAbortedError(10053, '你的主机中的软件中止了一个已建立的连接。', None, 10053, None)), '(Request ID: 22d32c3a-eec7-4afe-bfec-8ed3936a91ba)')' thrown while requesting HEAD https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507/resolve/768f209d9ea81521153ed38c47d515654e938aea/merges.txt\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionAbortedError(10053, '你的主机中的软件中止了一个已建立的连接。', None, 10053, None)), '(Request ID: 5481eee8-7b74-420f-b55d-497a249a7dcf)')' thrown while requesting HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen3-4B-Thinking-2507/768f209d9ea81521153ed38c47d515654e938aea/generation_config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionAbortedError(10053, '你的主机中的软件中止了一个已建立的连接。', None, 10053, None)), '(Request ID: ebe71b15-ccc6-4af6-9f48-d191a6b3bec0)')' thrown while requesting HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen3-4B-Thinking-2507/768f209d9ea81521153ed38c47d515654e938aea/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionAbortedError(10053, '你的主机中的软件中止了一个已建立的连接。', None, 10053, None)), '(Request ID: dc45b461-c093-478a-86c3-b7445cdc1fa6)')' thrown while requesting HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen3-4B-Thinking-2507/768f209d9ea81521153ed38c47d515654e938aea/config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionAbortedError(10053, '你的主机中的软件中止了一个已建立的连接。', None, 10053, None)), '(Request ID: a4e18916-74be-41bc-bc5b-d5aac17383ea)')' thrown while requesting HEAD https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507/resolve/768f209d9ea81521153ed38c47d515654e938aea/README.md\n",
      "Retrying in 4s [Retry 3/5].\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None)), '(Request ID: 4e0e0d88-4467-4d22-8e22-b51dff3810b9)')' thrown while requesting GET https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507/resolve/768f209d9ea81521153ed38c47d515654e938aea/model-00002-of-00003.safetensors\n",
      "Retrying in 1s [Retry 1/5].\n",
      "Fetching 13 files:  15%|█▌        | 2/13 [00:05<00:24,  2.19s/it]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None)), '(Request ID: 06bf50f8-56b4-42a4-a77b-28576ca5150e)')' thrown while requesting GET https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507/resolve/768f209d9ea81521153ed38c47d515654e938aea/model-00001-of-00003.safetensors\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionAbortedError(10053, '你的主机中的软件中止了一个已建立的连接。', None, 10053, None)), '(Request ID: 3be0268d-3fdd-41f6-a30d-3cdf340734f8)')' thrown while requesting GET https://huggingface.co/api/resolve-cache/models/Qwen/Qwen3-4B-Thinking-2507/768f209d9ea81521153ed38c47d515654e938aea/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionAbortedError(10053, '你的主机中的软件中止了一个已建立的连接。', None, 10053, None)), '(Request ID: cf414c33-a779-4d6c-986e-334132c050c9)')' thrown while requesting HEAD https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507/resolve/768f209d9ea81521153ed38c47d515654e938aea/README.md\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionAbortedError(10053, '你的主机中的软件中止了一个已建立的连接。', None, 10053, None)), '(Request ID: e8895bdd-79e4-411b-ae72-480001c401d9)')' thrown while requesting HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen3-4B-Thinking-2507/768f209d9ea81521153ed38c47d515654e938aea/model.safetensors.index.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None)), '(Request ID: 87dde456-f28d-44ef-82f3-7879c91a0659)')' thrown while requesting GET https://huggingface.co/api/resolve-cache/models/Qwen/Qwen3-4B-Thinking-2507/768f209d9ea81521153ed38c47d515654e938aea/config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None)), '(Request ID: 2ccb1365-fe4c-4a02-b5fe-c3f532afe6e4)')' thrown while requesting GET https://huggingface.co/api/resolve-cache/models/Qwen/Qwen3-4B-Thinking-2507/768f209d9ea81521153ed38c47d515654e938aea/generation_config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None)), '(Request ID: 81346614-68e5-4979-8275-dd941976f429)')' thrown while requesting GET https://huggingface.co/api/resolve-cache/models/Qwen/Qwen3-4B-Thinking-2507/768f209d9ea81521153ed38c47d515654e938aea/config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None)), '(Request ID: 92a3aeea-4b77-4066-81a2-794626e9e19b)')' thrown while requesting HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen3-4B-Thinking-2507/768f209d9ea81521153ed38c47d515654e938aea/model.safetensors.index.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 0f28d235-5b89-4be6-8a02-64cfe312258f)')' thrown while requesting HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen3-4B-Thinking-2507/768f209d9ea81521153ed38c47d515654e938aea/merges.txt\n",
      "Retrying in 1s [Retry 1/5].\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "'(ProtocolError('Connection aborted.', ConnectionAbortedError(10053, '你的主机中的软件中止了一个已建立的连接。', None, 10053, None)), '(Request ID: 527b439b-8acd-460b-9c6e-e8b8bc909d15)')' thrown while requesting HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen3-4B-Thinking-2507/768f209d9ea81521153ed38c47d515654e938aea/merges.txt\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionAbortedError(10053, '你的主机中的软件中止了一个已建立的连接。', None, 10053, None)), '(Request ID: 9b402dd9-2127-4f1a-b739-547e2e3f3008)')' thrown while requesting GET https://huggingface.co/api/resolve-cache/models/Qwen/Qwen3-4B-Thinking-2507/768f209d9ea81521153ed38c47d515654e938aea/config.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionAbortedError(10053, '你的主机中的软件中止了一个已建立的连接。', None, 10053, None)), '(Request ID: eff420b3-4fb9-4bfe-8f07-3d84765a89eb)')' thrown while requesting HEAD https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507/resolve/768f209d9ea81521153ed38c47d515654e938aea/README.md\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionAbortedError(10053, '你的主机中的软件中止了一个已建立的连接。', None, 10053, None)), '(Request ID: 4de88b3d-80a4-4e99-a454-f3ae0a5d4d31)')' thrown while requesting HEAD https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507/resolve/768f209d9ea81521153ed38c47d515654e938aea/tokenizer_config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None)), '(Request ID: 26762bb6-d91d-4496-822a-bef2af0adec7)')' thrown while requesting HEAD https://huggingface.co/api/resolve-cache/models/Qwen/Qwen3-4B-Thinking-2507/768f209d9ea81521153ed38c47d515654e938aea/merges.txt\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionAbortedError(10053, '你的主机中的软件中止了一个已建立的连接。', None, 10053, None)), '(Request ID: 6a846b26-bfa8-4463-aedc-4cd3405fff1b)')' thrown while requesting HEAD https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507/resolve/768f209d9ea81521153ed38c47d515654e938aea/README.md\n",
      "Fetching 13 files:  15%|█▌        | 2/13 [00:25<02:17, 12.54s/it]\n",
      "'(ProtocolError('Connection aborted.', ConnectionAbortedError(10053, '你的主机中的软件中止了一个已建立的连接。', None, 10053, None)), '(Request ID: 36e36e5b-93ce-4b09-8d94-d78623c6d108)')' thrown while requesting GET https://huggingface.co/api/resolve-cache/models/Qwen/Qwen3-4B-Thinking-2507/768f209d9ea81521153ed38c47d515654e938aea/merges.txt\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionAbortedError(10053, '你的主机中的软件中止了一个已建立的连接。', None, 10053, None)), '(Request ID: af95df5e-6696-4b9d-8cb7-d982d9220858)')' thrown while requesting GET https://huggingface.co/api/resolve-cache/models/Qwen/Qwen3-4B-Thinking-2507/768f209d9ea81521153ed38c47d515654e938aea/merges.txt\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionAbortedError(10053, '你的主机中的软件中止了一个已建立的连接。', None, 10053, None)), '(Request ID: b5599293-144e-4891-b2c5-41e2832f5067)')' thrown while requesting GET https://huggingface.co/api/resolve-cache/models/Qwen/Qwen3-4B-Thinking-2507/768f209d9ea81521153ed38c47d515654e938aea/vocab.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionAbortedError(10053, '你的主机中的软件中止了一个已建立的连接。', None, 10053, None)), '(Request ID: 0d987941-c2a0-4630-8d5a-fc9cf9c41de5)')' thrown while requesting GET https://huggingface.co/api/resolve-cache/models/Qwen/Qwen3-4B-Thinking-2507/768f209d9ea81521153ed38c47d515654e938aea/merges.txt\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None)), '(Request ID: a6ef4a58-adea-4fef-aa31-3cbb373381c2)')' thrown while requesting GET https://huggingface.co/api/resolve-cache/models/Qwen/Qwen3-4B-Thinking-2507/768f209d9ea81521153ed38c47d515654e938aea/config.json\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None)), '(Request ID: fe7d4a5b-2eb2-40bc-9a64-fbdfa13f47a7)')' thrown while requesting GET https://huggingface.co/api/resolve-cache/models/Qwen/Qwen3-4B-Thinking-2507/768f209d9ea81521153ed38c47d515654e938aea/config.json\n",
      "Error while downloading from https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507/resolve/768f209d9ea81521153ed38c47d515654e938aea/model-00002-of-00003.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None)), '(Request ID: 16cb606d-43f2-4cb3-9404-3a5caa2e60d9)')' thrown while requesting GET https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507/resolve/768f209d9ea81521153ed38c47d515654e938aea/model-00002-of-00003.safetensors\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionAbortedError(10053, '你的主机中的软件中止了一个已建立的连接。', None, 10053, None)), '(Request ID: 2f002961-3723-42d4-a0f9-037d3a23bdbf)')' thrown while requesting GET https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507/resolve/768f209d9ea81521153ed38c47d515654e938aea/model-00002-of-00003.safetensors\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None)), '(Request ID: 1b4e06fa-01f1-4dcf-af1d-14bf5089b52c)')' thrown while requesting GET https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507/resolve/768f209d9ea81521153ed38c47d515654e938aea/model-00002-of-00003.safetensors\n",
      "Retrying in 4s [Retry 3/5].\n",
      "Error while downloading from https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507/resolve/768f209d9ea81521153ed38c47d515654e938aea/model-00003-of-00003.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None)), '(Request ID: fb7d3383-2ae7-4a9c-9dc2-42cb39115a29)')' thrown while requesting GET https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507/resolve/768f209d9ea81521153ed38c47d515654e938aea/model-00003-of-00003.safetensors\n",
      "Retrying in 1s [Retry 1/5].\n",
      "Error while downloading from https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507/resolve/768f209d9ea81521153ed38c47d515654e938aea/model-00001-of-00003.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None)), '(Request ID: 1f94f1ba-1c0a-4d47-b26d-d7845aeec36e)')' thrown while requesting GET https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507/resolve/768f209d9ea81521153ed38c47d515654e938aea/model-00001-of-00003.safetensors\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionAbortedError(10053, '你的主机中的软件中止了一个已建立的连接。', None, 10053, None)), '(Request ID: 50becd82-5e39-45ca-a390-77c2e3e964fa)')' thrown while requesting GET https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507/resolve/768f209d9ea81521153ed38c47d515654e938aea/model-00001-of-00003.safetensors\n",
      "Retrying in 2s [Retry 2/5].\n",
      "Error while downloading from https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507/resolve/768f209d9ea81521153ed38c47d515654e938aea/model-00003-of-00003.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "Error while downloading from https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507/resolve/768f209d9ea81521153ed38c47d515654e938aea/model-00001-of-00003.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "Error while downloading from https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507/resolve/768f209d9ea81521153ed38c47d515654e938aea/model-00002-of-00003.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None)), '(Request ID: ed5af5ca-0d33-48df-84f5-794b75a191c6)')' thrown while requesting GET https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507/resolve/768f209d9ea81521153ed38c47d515654e938aea/model-00003-of-00003.safetensors\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionAbortedError(10053, '你的主机中的软件中止了一个已建立的连接。', None, 10053, None)), '(Request ID: 795ffc59-5239-4af9-aa55-2512e9525eee)')' thrown while requesting GET https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507/resolve/768f209d9ea81521153ed38c47d515654e938aea/model-00001-of-00003.safetensors\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import re\n",
    "import json\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from llm_utils import LocalLLMClient, setup_logging\n",
    "from whitebox_calibration import calibrate_expert_prob, find_optimal_temperature\n",
    "\n",
    "# ============== 配置 ==============\n",
    "setup_logging(level=\"INFO\")\n",
    "\n",
    "# 创建全局本地LLM客户端（白盒模型）\n",
    "client = LocalLLMClient(\n",
    "    model_id=\"Qwen/Qwen3-4B-Thinking-2507\",  # 本地模型\n",
    "    local_path=\"./models/qwen3-4b-thinking\",\n",
    "    device=\"auto\",  # 自动选择GPU/CPU\n",
    "    max_tokens=200,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# === 工具函数 ===\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "def get_yes_no_probabilities_from_distribution(token_distribution: Dict[str, float]) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    从token概率分布中提取Yes/No的概率\n",
    "    \"\"\"\n",
    "    # 初始化概率\n",
    "    yes_prob = 0.0\n",
    "    no_prob = 0.0\n",
    "    \n",
    "    # 查找各种形式的Yes/No token\n",
    "    yes_tokens = [\"Yes\", \"yes\", \"YES\", \"是\", \"对的\", \"正确\", \"True\", \"true\"]\n",
    "    no_tokens = [\"No\", \"no\", \"NO\", \"否\", \"不对\", \"错误\", \"False\", \"false\"]\n",
    "    \n",
    "    # 累加所有Yes相关token的概率\n",
    "    for token in yes_tokens:\n",
    "        if token in token_distribution:\n",
    "            yes_prob += token_distribution[token]\n",
    "    \n",
    "    # 累加所有No相关token的概率\n",
    "    for token in no_tokens:\n",
    "        if token in token_distribution:\n",
    "            no_prob += token_distribution[token]\n",
    "    \n",
    "    # 如果没有找到任何Yes/No token，检查子串\n",
    "    if yes_prob == 0 and no_prob == 0:\n",
    "        for token, prob in token_distribution.items():\n",
    "            token_lower = token.lower()\n",
    "            if any(yes_word in token_lower for yes_word in [\"yes\", \"是\", \"对\", \"true\"]):\n",
    "                yes_prob += prob\n",
    "            elif any(no_word in token_lower for no_word in [\"no\", \"否\", \"错\", \"false\"]):\n",
    "                no_prob += prob\n",
    "    \n",
    "    # 如果还是没有找到，尝试归一化第一个token的概率\n",
    "    if yes_prob == 0 and no_prob == 0 and token_distribution:\n",
    "        # 取前两个token的概率\n",
    "        sorted_tokens = sorted(token_distribution.items(), key=lambda x: x[1], reverse=True)\n",
    "        if len(sorted_tokens) >= 2:\n",
    "            # 假设第一个token是\"Yes\"或\"No\"\n",
    "            # 这里我们无法判断，返回0.5/0.5\n",
    "            return 0.5, 0.5\n",
    "    \n",
    "    # 归一化概率\n",
    "    total = yes_prob + no_prob\n",
    "    if total > 0:\n",
    "        yes_prob_normalized = yes_prob / total\n",
    "        no_prob_normalized = no_prob / total\n",
    "    else:\n",
    "        yes_prob_normalized = 0.5\n",
    "        no_prob_normalized = 0.5\n",
    "    \n",
    "    return yes_prob_normalized, no_prob_normalized\n",
    "\n",
    "def llm_judge_with_token_prob(prompt, x1, x2, n_sample=5, method='token_prob'):\n",
    "    \"\"\"\n",
    "    使用白盒LLM进行判断，基于token概率\n",
    "    \n",
    "    参数:\n",
    "        prompt: 提示词\n",
    "        x1, x2: 变量名\n",
    "        n_sample: 采样次数（用于frequency模拟）\n",
    "        method: 'token_prob' | 'frequency' | 'logit' (保持接口兼容性，实际都使用token概率)\n",
    "    \"\"\"\n",
    "    if method == 'frequency':\n",
    "        # 频率方法：多次采样，统计Yes/No出现频率\n",
    "        votes = []\n",
    "        yes_probs = []\n",
    "        no_probs = []\n",
    "        \n",
    "        for _ in range(n_sample):\n",
    "            try:\n",
    "                # 调用本地模型，获取token概率\n",
    "                response = client.chat(\n",
    "                    prompt,\n",
    "                    return_token_probs=True,\n",
    "                    temperature=0.7  # 添加随机性\n",
    "                )\n",
    "                \n",
    "                # 裁剪思考链\n",
    "                cropped_response = response.crop_thinking()\n",
    "                \n",
    "                # 获取第一个token的概率分布\n",
    "                token_dist = client.get_token_distributions(\n",
    "                    cropped_response,\n",
    "                    top_k=20,\n",
    "                    skip_thinking=True,\n",
    "                    skip_zeros=True,\n",
    "                    zero_threshold=0.001\n",
    "                )\n",
    "                \n",
    "                if len(token_dist) > 0:\n",
    "                    # 从概率分布中提取Yes/No概率\n",
    "                    yes_prob, no_prob = get_yes_no_probabilities_from_distribution(token_dist[0])\n",
    "                    yes_probs.append(yes_prob)\n",
    "                    no_probs.append(no_prob)\n",
    "                    \n",
    "                    # 根据概率投票\n",
    "                    vote = 1 if yes_prob >= no_prob else 0\n",
    "                    votes.append(vote)\n",
    "            except Exception as e:\n",
    "                print(f\"采样失败: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not votes:\n",
    "            p_yes = 0.5\n",
    "            p_no = 0.5\n",
    "        else:\n",
    "            p_yes = sum(votes) / len(votes)\n",
    "            p_no = 1 - p_yes\n",
    "        \n",
    "        # 计算平均概率\n",
    "        avg_yes_prob = sum(yes_probs) / len(yes_probs) if yes_probs else p_yes\n",
    "        avg_no_prob = sum(no_probs) / len(no_probs) if no_probs else p_no\n",
    "        \n",
    "        if p_yes >= p_no:\n",
    "            return {\"label\": 1, \"prob\": avg_yes_prob, \"yes_prob\": avg_yes_prob, \"no_prob\": avg_no_prob}\n",
    "        else:\n",
    "            return {\"label\": 0, \"prob\": avg_no_prob, \"yes_prob\": avg_yes_prob, \"no_prob\": avg_no_prob}\n",
    "    \n",
    "    elif method == 'logit':\n",
    "        # Logit方法：解析logits并计算概率\n",
    "        try:\n",
    "            response = client.chat(prompt, return_token_probs=True)\n",
    "            text = response.content.strip()\n",
    "            \n",
    "            # 尝试从文本中解析logits\n",
    "            matches = re.findall(r\"([-]?[0-9]*\\.?[0-9]+)\", text)\n",
    "            if len(matches) >= 2:\n",
    "                l1, l2 = float(matches[0]), float(matches[1])\n",
    "                logit_yes, logit_no = l1, l2\n",
    "            else:\n",
    "                # 如果无法解析，使用token概率\n",
    "                return llm_judge_with_token_prob(prompt, x1, x2, n_sample=1, method='token_prob')\n",
    "            \n",
    "            p_yes = sigmoid(logit_yes)\n",
    "            p_no = sigmoid(logit_no)\n",
    "            \n",
    "            # 归一化\n",
    "            total = p_yes + p_no\n",
    "            if total > 0:\n",
    "                p_yes_normalized = p_yes / total\n",
    "                p_no_normalized = p_no / total\n",
    "            else:\n",
    "                p_yes_normalized = 0.5\n",
    "                p_no_normalized = 0.5\n",
    "            \n",
    "            if p_yes_normalized >= p_no_normalized:\n",
    "                return {\"label\": 1, \"prob\": p_yes_normalized, \"yes_prob\": p_yes_normalized, \"no_prob\": p_no_normalized}\n",
    "            else:\n",
    "                return {\"label\": 0, \"prob\": p_no_normalized, \"yes_prob\": p_yes_normalized, \"no_prob\": p_no_normalized}\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Logit方法失败: {e}, 回退到token_prob方法\")\n",
    "            return llm_judge_with_token_prob(prompt, x1, x2, n_sample=1, method='token_prob')\n",
    "    \n",
    "    else:  # token_prob 方法\n",
    "        # 使用白盒模型的token概率\n",
    "        all_yes_probs = []\n",
    "        all_no_probs = []\n",
    "        \n",
    "        for i in range(max(1, n_sample)):  # 至少采样1次\n",
    "            try:\n",
    "                # 调用本地模型，获取token概率\n",
    "                response = client.chat(\n",
    "                    prompt,\n",
    "                    return_token_probs=True,\n",
    "                    temperature=0.1 if n_sample == 1 else 0.7  # 单次采样降低随机性\n",
    "                )\n",
    "                \n",
    "                # 裁剪思考链\n",
    "                cropped_response = response.crop_thinking()\n",
    "                \n",
    "                # 获取第一个token的概率分布\n",
    "                token_dist = client.get_token_distributions(\n",
    "                    cropped_response,\n",
    "                    top_k=20,\n",
    "                    skip_thinking=True,\n",
    "                    skip_zeros=True,\n",
    "                    zero_threshold=0.001\n",
    "                )\n",
    "                \n",
    "                if len(token_dist) > 0:\n",
    "                    # 从概率分布中提取Yes/No概率\n",
    "                    yes_prob, no_prob = get_yes_no_probabilities_from_distribution(token_dist[0])\n",
    "                    \n",
    "                    all_yes_probs.append(yes_prob)\n",
    "                    all_no_probs.append(no_prob)\n",
    "                    \n",
    "                    if n_sample > 1:\n",
    "                        print(f\"  采样{i+1}: P(Yes)={yes_prob:.4f}, P(No)={no_prob:.4f}\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"采样{i+1}失败: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # 如果没有成功采样，使用默认值\n",
    "        if not all_yes_probs:\n",
    "            avg_yes_prob = 0.5\n",
    "            avg_no_prob = 0.5\n",
    "        else:\n",
    "            # 计算平均概率\n",
    "            avg_yes_prob = sum(all_yes_probs) / len(all_yes_probs)\n",
    "            avg_no_prob = sum(all_no_probs) / len(all_no_probs)\n",
    "        \n",
    "        if n_sample > 1:\n",
    "            print(f\"  平均概率: P(Yes)={avg_yes_prob:.4f}, P(No)={avg_no_prob:.4f}\")\n",
    "        \n",
    "        # 根据平均概率决定标签\n",
    "        if avg_yes_prob >= avg_no_prob:\n",
    "            return {\"label\": 1, \"prob\": avg_yes_prob, \"yes_prob\": avg_yes_prob, \"no_prob\": avg_no_prob}\n",
    "        else:\n",
    "            return {\"label\": 0, \"prob\": avg_no_prob, \"yes_prob\": avg_yes_prob, \"no_prob\": avg_no_prob}\n",
    "\n",
    "# === MoE 专家定义 ===\n",
    "CAUSAL_EXPERTS = {\n",
    "    \"graph_theory\": {\n",
    "        \"name\": \"因果图论专家\",\n",
    "        \"description\": \"专门分析因果图结构，精通d-分离、路径阻断、后门准则等图论概念。推理方式：系统检查所有可能的路径，分析路径上的变量类型和连接方式，使用严谨的图论推理链条。\",\n",
    "        \"specialty\": \"路径分析、环路检测、d-分离判断\",\n",
    "        \"reasoning_style\": \"结构化图遍历\",\n",
    "        \"output_format\": \"基于图结构的二值判断\"\n",
    "    },\n",
    "    \"statistical\": {\n",
    "        \"name\": \"计量统计专家\", \n",
    "        \"description\": \"专注于统计检验和概率独立性分析，擅长相关性分析、条件独立性检验、混淆变量检测。推理方式：考虑样本分布、统计显著性、置信区间等统计概念。\",\n",
    "        \"specialty\": \"独立性检验、相关性分析、混淆检测\",\n",
    "        \"reasoning_style\": \"概率统计推理\",\n",
    "        \"output_format\": \"基于统计证据的概率判断\"\n",
    "    },\n",
    "    \"domain_knowledge\": {\n",
    "        \"name\": \"领域先验专家\",\n",
    "        \"description\": \"基于现实世界知识和科学常识进行因果推理，考虑时间顺序、物理可能性、生物学机制等约束。推理方式：结合文献证据、科学理论和常识性约束。\",\n",
    "        \"specialty\": \"机制分析、时序推理、现实约束\",\n",
    "        \"reasoning_style\": \"基于证据的归纳推理\", \n",
    "        \"output_format\": \"基于领域知识的合理性判断\"\n",
    "    },\n",
    "    \"counterfactual\": {\n",
    "        \"name\": \"反事实干预专家\",\n",
    "        \"description\": \"从干预和潜在结果角度分析因果关系，考虑do-calculus、随机化实验理想情况。推理方式：构建反事实场景，分析干预后的可能变化。\",\n",
    "        \"specialty\": \"干预分析、潜在结果、do算子\",\n",
    "        \"reasoning_style\": \"反事实思维实验\",\n",
    "        \"output_format\": \"基于干预推理的因果判断\"\n",
    "    },\n",
    "    \"temporal_dynamics\": {\n",
    "        \"name\": \"时间动态专家\",\n",
    "        \"description\": \"专门分析时间顺序和动态过程，强调原因必须发生在结果之前，考虑延迟效应和动态反馈。推理方式：严格检查时间顺序，分析因果链的时间特性。\",\n",
    "        \"specialty\": \"时序分析、动态过程、延迟效应\",\n",
    "        \"reasoning_style\": \"时间序列推理\", \n",
    "        \"output_format\": \"基于时间顺序的因果判断\"\n",
    "    },\n",
    "    \"mechanism_modeling\": {\n",
    "        \"name\": \"机制建模专家\", \n",
    "        \"description\": \"专注于因果机制的可解释性建模，分析中间变量、中介效应和机制路径。推理方式：构建机制框图，分析变量间的功能关系。\",\n",
    "        \"specialty\": \"中介分析、机制路径、功能关系\",\n",
    "        \"reasoning_style\": \"机制分解建模\",\n",
    "        \"output_format\": \"基于机制完整性的判断\"\n",
    "    },\n",
    "    \"robustness_analysis\": {\n",
    "        \"name\": \"稳健性检验专家\",\n",
    "        \"description\": \"专门评估因果关系的稳健性和敏感性，考虑不同假设下的结果稳定性。推理方式：进行敏感性分析，检验边界条件和假设变化的影响。\",\n",
    "        \"specialty\": \"敏感性分析、稳健检验、边界情况\",\n",
    "        \"reasoning_style\": \"多情景验证\",\n",
    "        \"output_format\": \"基于稳健性评估的判断\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# === Router 函数（增强版）===\n",
    "def expert_router(question_type: str, x1: str, x2: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    根据问题类型和变量特征选择最相关的专家\n",
    "    \"\"\"\n",
    "    # 基础路由规则\n",
    "    routing_rules = {\n",
    "        \"backdoor_path\": [\n",
    "            \"graph_theory\", \"statistical\", \"counterfactual\", \"temporal_dynamics\", \n",
    "            \"mechanism_modeling\", \"robustness_analysis\", \"domain_knowledge\"\n",
    "        ],\n",
    "        \"independence\": [\n",
    "            \"statistical\", \"graph_theory\", \"counterfactual\", \"robustness_analysis\",\n",
    "            \"temporal_dynamics\", \"mechanism_modeling\", \"domain_knowledge\"\n",
    "        ],\n",
    "        \"latent_confounder\": [\n",
    "            \"domain_knowledge\", \"statistical\", \"mechanism_modeling\", \"counterfactual\",\n",
    "            \"robustness_analysis\", \"graph_theory\", \"temporal_dynamics\"\n",
    "        ],\n",
    "        \"causal_direction\": [\n",
    "            \"temporal_dynamics\", \"domain_knowledge\", \"counterfactual\", \"mechanism_modeling\",\n",
    "            \"statistical\", \"graph_theory\", \"robustness_analysis\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # 获取基础专家列表\n",
    "    base_experts = routing_rules.get(question_type, list(CAUSAL_EXPERTS.keys()))\n",
    "    \n",
    "    # 使用门诊LLM agent来智能选择专家\n",
    "    try:\n",
    "        clinic_recommendation = clinic_agent_recommend(question_type, x1, x2, base_experts)\n",
    "        return clinic_recommendation\n",
    "    except Exception as e:\n",
    "        print(f\"门诊agent路由失败: {e}，使用基础路由\")\n",
    "        # 失败时返回基础专家列表的前3个\n",
    "        return base_experts[:3]\n",
    "\n",
    "def clinic_agent_recommend(question_type: str, x1: str, x2: str, base_experts: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    门诊LLM agent：根据具体变量和问题类型推荐最相关的专家\n",
    "    \"\"\"\n",
    "    # 使用独立的LLM客户端进行推荐\n",
    "    clinic_client = LocalLLMClient(\n",
    "        model_id=\"Qwen/Qwen3-4B-Thinking-2507\",\n",
    "        local_path=\"./models/qwen3-4b-thinking\",\n",
    "        device=\"auto\",\n",
    "        max_tokens=200,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    \n",
    "    # 构建专家选择提示\n",
    "    experts_description = \"\\n\".join([\n",
    "        f\"- {expert}: {CAUSAL_EXPERTS[expert]['description']}\" \n",
    "        for expert in base_experts\n",
    "    ])\n",
    "    \n",
    "    clinic_prompt = f\"\"\"\n",
    "作为因果推断门诊专家，你需要为以下因果分析任务选择最合适的专家组合：\n",
    "\n",
    "**分析任务**: {question_type}\n",
    "**变量对**: {x1} 和 {x2}\n",
    "\n",
    "**可用专家列表**:\n",
    "{experts_description}\n",
    "\n",
    "**选择要求**:\n",
    "1. 根据变量内容和问题类型，选择最相关的3个专家\n",
    "2. 按相关性从高到低排序\n",
    "3. 确保专家视角的多样性（不要选择推理方式相似的专家）\n",
    "4. 考虑变量的领域特性（医学、经济、社会等）\n",
    "\n",
    "请按照以下格式输出：\n",
    "最终推荐专家: 专家1, 专家2, 专家3\n",
    "\n",
    "**注意**: 只输出专家名称，用逗号分隔，不要添加其他文字。\n",
    "\"\"\"\n",
    "    \n",
    "    # 调用LLM获取推荐\n",
    "    response = clinic_client.chat(clinic_prompt)\n",
    "    response_text = response.content.strip()\n",
    "    print(f\"门诊agent原始响应: {response_text}\")\n",
    "    \n",
    "    # 解析返回的专家列表\n",
    "    recommended_experts = parse_clinic_recommendation(response_text, base_experts)\n",
    "    \n",
    "    print(f\"门诊agent推荐: {recommended_experts}\")\n",
    "    \n",
    "    return recommended_experts\n",
    "\n",
    "def parse_clinic_recommendation(response_text: str, base_experts: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    解析门诊agent的推荐结果\n",
    "    \"\"\"\n",
    "    # 方法1: 查找\"最终推荐专家\"后的内容\n",
    "    if \"最终推荐专家\" in response_text:\n",
    "        parts = response_text.split(\"最终推荐专家\")\n",
    "        if len(parts) > 1:\n",
    "            expert_line = parts[1].strip().lstrip(\":\").strip()\n",
    "            return extract_experts_from_line(expert_line, base_experts)\n",
    "    \n",
    "    # 方法2: 查找最后一行\n",
    "    lines = [line.strip() for line in response_text.split('\\n') if line.strip()]\n",
    "    if lines:\n",
    "        last_line = lines[-1]\n",
    "        experts = extract_experts_from_line(last_line, base_experts)\n",
    "        if len(experts) >= 2:\n",
    "            return experts\n",
    "    \n",
    "    # 方法3: 在整个文本中搜索专家名称\n",
    "    found_experts = []\n",
    "    for expert in base_experts:\n",
    "        if expert in response_text:\n",
    "            found_experts.append(expert)\n",
    "    \n",
    "    if len(found_experts) >= 2:\n",
    "        return found_experts[:3]  # 取前3个找到的专家\n",
    "    \n",
    "    # 如果所有方法都失败，返回基础专家前3个\n",
    "    print(f\"门诊agent解析不充分，使用基础专家: {base_experts[:3]}\")\n",
    "    return base_experts[:3]\n",
    "\n",
    "def extract_experts_from_line(line: str, base_experts: List[str]) -> List[str]:\n",
    "    \"\"\"从一行文本中提取专家名称\"\"\"\n",
    "    experts = []\n",
    "    \n",
    "    # 清理行内容\n",
    "    clean_line = line.replace('：', ':').replace('，', ',').replace(' ', '')\n",
    "    \n",
    "    # 多种分割方式尝试\n",
    "    separators = [',', '、', ';', '，']\n",
    "    \n",
    "    for sep in separators:\n",
    "        if sep in clean_line:\n",
    "            parts = [part.strip() for part in clean_line.split(sep)]\n",
    "            break\n",
    "    else:\n",
    "        parts = [clean_line]\n",
    "    \n",
    "    for part in parts:\n",
    "        clean_part = part.lower().replace('专家', '').replace('expert', '').strip()\n",
    "        \n",
    "        # 直接匹配专家名称\n",
    "        for expert in base_experts:\n",
    "            if (expert in clean_part or \n",
    "                expert.replace('_', ' ') in clean_part or\n",
    "                CAUSAL_EXPERTS[expert]['name'] in part):\n",
    "                if expert not in experts:\n",
    "                    experts.append(expert)\n",
    "                    break\n",
    "        \n",
    "        if len(experts) >= 3:\n",
    "            break\n",
    "    \n",
    "    return experts\n",
    "\n",
    "# === 专家提示创建函数 ===\n",
    "def create_expert_prompt(base_prompt: str, expert_type: str, x1: str, x2: str) -> str:\n",
    "    \"\"\"\n",
    "    为不同专家创建专业化的prompt\n",
    "    \"\"\"\n",
    "    expert_info = CAUSAL_EXPERTS[expert_type]\n",
    "    \n",
    "    expert_specific_prompts = {\n",
    "        \"graph_theory\": f\"\"\"作为{expert_info['name']}，请严格遵循以下专业分析框架：\n",
    "\n",
    "{expert_info['description']}\n",
    "\n",
    "专业特长：{expert_info['specialty']}\n",
    "推理风格：{expert_info['reasoning_style']}\n",
    "输出要求：基于图结构的二值判断\n",
    "\n",
    "分析步骤：\n",
    "1. 构建因果图模型，识别所有可能的路径\n",
    "2. 应用d-分离准则分析路径阻塞情况  \n",
    "3. 检查后门路径、前门路径和混杂路径\n",
    "4. 基于图结构做出明确的二值判断\n",
    "\n",
    "请严格按照图论原理进行分析，直接输出是或否（Yes/No）。\\n\\n{base_prompt}\"\"\",\n",
    "\n",
    "        \"statistical\": f\"\"\"作为{expert_info['name']}，请严格遵循以下专业分析框架：\n",
    "\n",
    "{expert_info['description']}\n",
    "\n",
    "专业特长：{expert_info['specialty']}\n",
    "推理风格：{expert_info['reasoning_style']}\n",
    "输出要求：基于统计证据的二值判断\n",
    "\n",
    "分析步骤：\n",
    "1. 评估变量间的统计相关性\n",
    "2. 考虑条件独立性和混淆因素\n",
    "3. 分析统计显著性和置信度\n",
    "4. 基于概率证据做出明确的二值判断\n",
    "\n",
    "请基于统计原理进行严谨分析，直接输出是或否（Yes/No）。\\n\\n{base_prompt}\"\"\",\n",
    "\n",
    "        \"domain_knowledge\": f\"\"\"作为{expert_info['name']}，请严格遵循以下专业分析框架：\n",
    "\n",
    "{expert_info['description']}\n",
    "\n",
    "专业特长：{expert_info['specialty']}\n",
    "推理风格：{expert_info['reasoning_style']}\n",
    "输出要求：基于领域知识的二值判断\n",
    "\n",
    "分析步骤：\n",
    "1. 调用相关领域的科学知识和常识\n",
    "2. 考虑物理/生物/社会机制的合理性\n",
    "3. 评估时间顺序和现实约束条件\n",
    "4. 基于先验知识做出明确的二值判断\n",
    "\n",
    "请结合现实世界知识进行推理，直接输出是或否（Yes/No）。\\n\\n{base_prompt}\"\"\",\n",
    "\n",
    "        \"counterfactual\": f\"\"\"作为{expert_info['name']}，请严格遵循以下专业分析框架：\n",
    "\n",
    "{expert_info['description']}\n",
    "\n",
    "专业特长：{expert_info['specialty']}\n",
    "推理风格：{expert_info['reasoning_style']}\n",
    "输出要求：基于干预推理的二值判断\n",
    "\n",
    "分析步骤：\n",
    "1. 构建干预场景（do-操作）\n",
    "2. 比较实际结果与反事实结果\n",
    "3. 分析潜在结果分布\n",
    "4. 基于干预效应做出明确的二值判断\n",
    "\n",
    "请使用反事实推理进行分析，直接输出是或否（Yes/No）。\\n\\n{base_prompt}\"\"\",\n",
    "\n",
    "        \"temporal_dynamics\": f\"\"\"作为{expert_info['name']}，请严格遵循以下专业分析框架：\n",
    "\n",
    "{expert_info['description']}\n",
    "\n",
    "专业特长：{expert_info['specialty']}\n",
    "推理风格：{expert_info['reasoning_style']}\n",
    "输出要求：基于时间顺序的二值判断\n",
    "\n",
    "分析步骤：\n",
    "1. 严格检查原因和结果的时间顺序\n",
    "2. 分析延迟效应和动态过程\n",
    "3. 考虑时间序列的因果结构\n",
    "4. 基于时间约束做出明确的二值判断\n",
    "\n",
    "请重点分析时间维度，直接输出是或否（Yes/No）。\\n\\n{base_prompt}\"\"\",\n",
    "\n",
    "        \"mechanism_modeling\": f\"\"\"作为{expert_info['name']}，请严格遵循以下专业分析框架：\n",
    "\n",
    "{expert_info['description']}\n",
    "\n",
    "专业特长：{expert_info['specialty']}\n",
    "推理风格：{expert_info['reasoning_style']}\n",
    "输出要求：基于机制完整性的二值判断\n",
    "\n",
    "分析步骤：\n",
    "1. 识别可能的中间机制和中介变量\n",
    "2. 分析因果链的功能完整性\n",
    "3. 评估机制路径的合理性\n",
    "4. 基于机制可解释性做出明确的二值判断\n",
    "\n",
    "请专注于机制分析，直接输出是或否（Yes/No）。\\n\\n{base_prompt}\"\"\",\n",
    "\n",
    "        \"robustness_analysis\": f\"\"\"作为{expert_info['name']}，请严格遵循以下专业分析框架：\n",
    "\n",
    "{expert_info['description']}\n",
    "\n",
    "专业特长：{expert_info['specialty']}\n",
    "推理风格：{expert_info['reasoning_style']}\n",
    "输出要求：基于稳健性评估的二值判断\n",
    "\n",
    "分析步骤：\n",
    "1. 测试不同假设条件下的结果稳定性\n",
    "2. 进行敏感性分析和边界检验\n",
    "3. 评估结论的稳健程度\n",
    "4. 基于稳健性评估做出明确的二值判断\n",
    "\n",
    "请重点分析结论的可靠性，直接输出是或否（Yes/No）。\\n\\n{base_prompt}\"\"\"\n",
    "    }\n",
    "    \n",
    "    return expert_specific_prompts.get(expert_type, base_prompt)\n",
    "\n",
    "# === MoE 集成函数 ===\n",
    "def aggregate_expert_judgments(expert_results: List[Dict]) -> Dict:\n",
    "    \"\"\"\n",
    "    整合多个专家的判断结果\n",
    "    \"\"\"\n",
    "    if not expert_results:\n",
    "        return {\"label\": 0, \"prob\": 0.5}\n",
    "    \n",
    "    # 简单加权平均\n",
    "    total_prob_yes = 0\n",
    "    total_weight = 0\n",
    "    \n",
    "    for result in expert_results:\n",
    "        weight = result.get(\"confidence\", 1.0)\n",
    "        # 使用yes_prob字段（从token概率得来）\n",
    "        prob_yes = result.get(\"calibrated_yes_prob\", result.get(\"yes_prob\", result[\"prob\"] if result[\"label\"] == 1 else 1 - result[\"prob\"]))\n",
    "        total_prob_yes += prob_yes * weight\n",
    "        total_weight += weight\n",
    "    \n",
    "    aggregated_prob_yes = total_prob_yes / total_weight if total_weight > 0 else 0.5\n",
    "    \n",
    "    if aggregated_prob_yes >= 0.5:\n",
    "        return {\"label\": 1, \"prob\": aggregated_prob_yes, \"yes_prob\": aggregated_prob_yes, \"no_prob\": 1 - aggregated_prob_yes}\n",
    "    else:\n",
    "        return {\"label\": 0, \"prob\": 1 - aggregated_prob_yes, \"yes_prob\": aggregated_prob_yes, \"no_prob\": 1 - aggregated_prob_yes}\n",
    "\n",
    "# === MoE判断函数 ===\n",
    "def run_step_with_moe(base_prompt: str, x1: str, x2: str, question_type: str, method: str = 'token_prob', true_labels: list = None) -> Dict:\n",
    "    \"\"\"\n",
    "    使用MoE架构运行单个判断步骤（新增校准参数）\n",
    "    \"\"\"\n",
    "    # 1. 路由选择专家\n",
    "    selected_experts = expert_router(question_type, x1, x2)\n",
    "    print(f\"为问题 '{question_type}' 选择的专家: {selected_experts}\")\n",
    "    \n",
    "    # 2. 执行专家判断\n",
    "    expert_results = []\n",
    "    expert_yes_probs = []  # 新增：收集原始Yes概率，用于找最优温度\n",
    "    for expert in selected_experts:\n",
    "        expert_prompt = create_expert_prompt(base_prompt, expert, x1, x2)\n",
    "        try:\n",
    "            # 使用本地白盒模型的token概率方法\n",
    "            result = llm_judge_with_token_prob(expert_prompt, x1, x2, n_sample=2, method=method)\n",
    "            result[\"expert\"] = expert\n",
    "            result[\"confidence\"] = 1.0\n",
    "            expert_results.append(result)\n",
    "            expert_yes_probs.append(result[\"yes_prob\"])  # 新增：收集原始概率\n",
    "            print(f\"专家 {expert} 判断完成: label={result['label']}, P(Yes)={result.get('yes_prob', result['prob']):.4f}, P(No)={result.get('no_prob', 1-result['prob']):.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"专家 {expert} 执行失败: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # 3. 如果没有专家成功，使用默认方法\n",
    "    if not expert_results:\n",
    "        print(\"所有专家执行失败，使用默认方法\")\n",
    "        return llm_judge_with_token_prob(base_prompt, x1, x2, n_sample=3, method=method)\n",
    "    \n",
    "    # ========== 新增：白箱校准逻辑 ==========\n",
    "    # 找最优温度（无真实标签时用1.0）\n",
    "    opt_temp = find_optimal_temperature(expert_yes_probs, true_labels)\n",
    "    print(f\"最优校准温度: {opt_temp}\")\n",
    "    \n",
    "    # 对每个专家的概率进行温度缩放校准\n",
    "    for result in expert_results:\n",
    "        yes_prob = result[\"yes_prob\"]\n",
    "        no_prob = result[\"no_prob\"]\n",
    "        # 校准概率\n",
    "        calib_yes, calib_no, entropy = calibrate_expert_prob(yes_prob, no_prob, opt_temp)\n",
    "        # 保存校准结果\n",
    "        result[\"calibrated_yes_prob\"] = calib_yes\n",
    "        result[\"calibrated_no_prob\"] = calib_no\n",
    "        result[\"entropy\"] = entropy  # 熵值（衡量不确定性）\n",
    "        result[\"calibration_temp\"] = opt_temp\n",
    "        # 打印校准日志\n",
    "        print(f\"专家 {result['expert']} 校准后: P(Yes)={calib_yes:.4f}, P(No)={calib_no:.4f}, 熵值={entropy:.4f}\")\n",
    "    # ======================================\n",
    "    \n",
    "    # 4. 整合专家意见（自动使用校准后的概率）\n",
    "    final_result = aggregate_expert_judgments(expert_results)\n",
    "    final_result[\"expert_results\"] = expert_results\n",
    "    # 新增：保存校准相关的汇总信息\n",
    "    final_result[\"calibration_temp\"] = opt_temp\n",
    "    final_result[\"avg_entropy\"] = np.mean([r[\"entropy\"] for r in expert_results]) if expert_results else 0.5\n",
    "    \n",
    "    print(f\"专家整合结果: label={final_result['label']}, P(Yes)={final_result['yes_prob']:.4f}, P(No)={final_result['no_prob']:.4f}\")\n",
    "    print(f\"校准汇总: 最优温度={opt_temp:.2f}, 平均熵值={final_result['avg_entropy']:.4f}\")\n",
    "    return final_result\n",
    "\n",
    "# === 具体判断函数（新增true_labels参数）===\n",
    "def check_backdoor(x1, x2, all_variables, method='token_prob', true_labels=None):\n",
    "    base_prompt = f\"\"\"在因果推断中，考虑以下所有变量：{all_variables}\n",
    "\n",
    "请判断在这些变量中，变量 {x1} 和 {x2} 之间是否存在 back-door path（后门路径）。\n",
    "\n",
    "后门路径是指从 {x1} 到 {x2} 的路径，其中包含指向 {x1} 的箭头，且这条路径没有被阻断。\n",
    "\n",
    "让我们一步步思考，然后直接输出是或否（Yes/No）。\"\"\"\n",
    "    return run_step_with_moe(base_prompt, x1, x2, \"backdoor_path\", method, true_labels=true_labels)\n",
    "\n",
    "def check_independence_after_block(x1, x2, all_variables, method='token_prob', true_labels=None):\n",
    "    base_prompt = f\"\"\"在因果推断中，考虑以下所有变量：{all_variables}\n",
    "\n",
    "如果阻断了 {x1} 和 {x2} 之间的所有 back-door path，那么 {x1} 与 {x2} 是否条件独立？\n",
    "\n",
    "让我们一步步思考，然后直接输出是或否（Yes/No）。\"\"\"\n",
    "    return run_step_with_moe(base_prompt, x1, x2, \"independence\", method, true_labels=true_labels)\n",
    "\n",
    "def check_latent_confounder_after_block(x1, x2, all_variables, method='token_prob', true_labels=None):\n",
    "    base_prompt = f\"\"\"在因果推断中，考虑以下所有变量：{all_variables}\n",
    "\n",
    "阻断了 {x1} 和 {x2} 之间的所有 back-door path 后，是否仍然存在未观察到的潜在混杂因子同时影响 {x1} 和 {x2}？\n",
    "\n",
    "让我们一步步思考，然后直接输出是或否（Yes/No）。\"\"\"\n",
    "    return run_step_with_moe(base_prompt, x1, x2, \"latent_confounder\", method, true_labels=true_labels)\n",
    "\n",
    "def check_causal_direction_after_block(x1, x2, all_variables, method='token_prob', true_labels=None):\n",
    "    base_prompt = f\"\"\"在因果推断中，考虑以下所有变量：{all_variables}\n",
    "\n",
    "阻断了 {x1} 和 {x2} 之间的所有 back-door path 后，请判断 {x1} 是否因果导致 {x2}？\n",
    "\n",
    "让我们一步步思考，然后直接输出是或否（Yes/No）。\"\"\"\n",
    "    return run_step_with_moe(base_prompt, x1, x2, \"causal_direction\", method, true_labels=true_labels)\n",
    "\n",
    "def check_independence(x1, x2, all_variables, method='token_prob', true_labels=None):\n",
    "    base_prompt = f\"\"\"在因果推断中，考虑以下所有变量：{all_variables}\n",
    "\n",
    "请判断变量 {x1} 和 {x2} 是否独立？\n",
    "\n",
    "让我们一步步思考，然后直接输出是或否（Yes/No）。\"\"\"\n",
    "    return run_step_with_moe(base_prompt, x1, x2, \"independence\", method, true_labels=true_labels)\n",
    "\n",
    "def check_latent_confounder(x1, x2, all_variables, method='token_prob', true_labels=None):\n",
    "    base_prompt = f\"\"\"在因果推断中，考虑以下所有变量：{all_variables}\n",
    "\n",
    "请判断变量 {x1} 和 {x2} 之间是否存在未观察到的潜在混杂因子？\n",
    "\n",
    "让我们一步步思考，然后直接输出是或否（Yes/No）。\"\"\"\n",
    "    return run_step_with_moe(base_prompt, x1, x2, \"latent_confounder\", method, true_labels=true_labels)\n",
    "\n",
    "def check_causal_direction(x1, x2, all_variables, method='token_prob', true_labels=None):\n",
    "    base_prompt = f\"\"\"在因果推断中，考虑以下所有变量：{all_variables}\n",
    "\n",
    "请判断 {x1} 是否因果导致 {x2}？\n",
    "\n",
    "让我们一步步思考，然后直接输出是或否（Yes/No）。\"\"\"\n",
    "    return run_step_with_moe(base_prompt, x1, x2, \"causal_direction\", method, true_labels=true_labels)\n",
    "\n",
    "def load_true_labels_from_json(json_path: str, x1: str, x2: str) -> dict:\n",
    "    \"\"\"\n",
    "    从JSON数据集（element1/element2/causal）读取真实标签，映射到树状推理步骤\n",
    "    \n",
    "    参数：\n",
    "        json_path: JSON文件路径（如\"./causal_500.json\"）\n",
    "        x1: 当前分析的变量1\n",
    "        x2: 当前分析的变量2\n",
    "    返回：\n",
    "        true_labels_dict: 树状推理各步骤的真实标签字典（仅映射因果方向步骤）\n",
    "    \"\"\"\n",
    "    # 1. 读取JSON文件\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)  # 假设JSON是列表格式：[{\"element1\":\"xxx\",\"element2\":\"xxx\",\"causal\":0}, ...]\n",
    "    \n",
    "    # 2. 匹配变量对（兼容element1/element2顺序颠倒）\n",
    "    matched_item = None\n",
    "    for item in data:\n",
    "        e1, e2 = item[\"element1\"], item[\"element2\"]\n",
    "        # 匹配当前x1/x2（顺序无关）\n",
    "        if (e1 == x1 and e2 == x2) or (e1 == x2 and e2 == x1):\n",
    "            matched_item = item\n",
    "            break\n",
    "    \n",
    "    # 3. 映射标签：JSON的causal → 树状推理的\"因果方向判断\"步骤\n",
    "    true_labels_dict = {}\n",
    "    if matched_item:\n",
    "        causal_label = matched_item[\"causal\"]  # 0/1\n",
    "        # 映射到树状推理的两个因果方向步骤（有后门路径/无后门路径）\n",
    "        true_labels_dict[\"x->y_after_block\"] = [causal_label]    # 有后门路径时的因果方向\n",
    "        true_labels_dict[\"x->y_no_backdoor\"] = [causal_label]    # 无后门路径时的因果方向\n",
    "        print(f\"✅ 从JSON匹配到{x1}-{x2}的真实因果标签：{causal_label}（1=x→y，0=y→x/无因果）\")\n",
    "    else:\n",
    "        print(f\"⚠️ 在JSON中未找到{x1}-{x2}的匹配数据，校准使用默认温度\")\n",
    "    \n",
    "    return true_labels_dict\n",
    "\n",
    "# === 完整的树查询函数 ===\n",
    "def tree_query(x1, x2, all_variables, method='token_prob', true_labels_dict: dict = None):\n",
    "    \"\"\"\n",
    "    基于树状逻辑的因果方向查询器（支持校准）\n",
    "    - true_labels_dict: 各步骤的真实标签，格式{\"backdoor_path\": [1/0], \"independent_after_block\": [1/0], ...}\n",
    "    \"\"\"\n",
    "    log = []\n",
    "\n",
    "    # Step 1: 是否存在 backdoor path?\n",
    "    print(\"=== Step 1: 检查后门路径 ===\")\n",
    "    # 传递对应步骤的真实标签（用于校准）\n",
    "    true_labels = true_labels_dict.get(\"backdoor_path\") if true_labels_dict else None\n",
    "    res_backdoor = check_backdoor(x1, x2, all_variables, method, true_labels=true_labels)\n",
    "    log.append((\"backdoor_path\", res_backdoor))\n",
    "\n",
    "    if res_backdoor[\"label\"] == 1:\n",
    "        print(\"存在后门路径，进入阻断后分析路径\")\n",
    "        # Step 2: 阻断路径后是否独立？\n",
    "        print(\"=== Step 2: 阻断后检查独立性 ===\")\n",
    "        true_labels = true_labels_dict.get(\"independent_after_block\") if true_labels_dict else None\n",
    "        res_ind = check_independence_after_block(x1, x2, all_variables, method, true_labels=true_labels)\n",
    "        log.append((\"independent_after_block\", res_ind))\n",
    "        if res_ind[\"label\"] == 1:\n",
    "            return {\"relation\": \"independent\", \"confidence\": res_ind[\"yes_prob\"], \"log\": log}\n",
    "\n",
    "        # Step 3: 是否存在潜在混杂因子？\n",
    "        print(\"=== Step 3: 检查潜在混杂因子 ===\")\n",
    "        true_labels = true_labels_dict.get(\"latent_confounder_after_block\") if true_labels_dict else None\n",
    "        res_latent = check_latent_confounder_after_block(x1, x2, all_variables, method, true_labels=true_labels)\n",
    "        log.append((\"latent_confounder_after_block\", res_latent))\n",
    "        if res_latent[\"label\"] == 1:\n",
    "            return {\"relation\": \"x<->y\", \"confidence\": res_latent[\"yes_prob\"], \"log\": log}\n",
    "\n",
    "        # Step 4: 判断方向 (x→y?)\n",
    "        print(\"=== Step 4: 判断因果方向 ===\")\n",
    "        true_labels = true_labels_dict.get(\"x->y_after_block\") if true_labels_dict else None\n",
    "        res_dir = check_causal_direction_after_block(x1, x2, all_variables, method, true_labels=true_labels)\n",
    "        log.append((\"x->y_after_block\", res_dir))\n",
    "        if res_dir[\"label\"] == 1:\n",
    "            return {\"relation\": \"x->y\", \"confidence\": res_dir[\"yes_prob\"], \"log\": log}\n",
    "        else:\n",
    "            return {\"relation\": \"y->x\", \"confidence\": res_dir[\"no_prob\"], \"log\": log}\n",
    "\n",
    "    else:\n",
    "        print(\"不存在后门路径，进入直接分析路径\")\n",
    "        # 不存在 backdoor path\n",
    "        true_labels = true_labels_dict.get(\"independent_no_backdoor\") if true_labels_dict else None\n",
    "        res_ind = check_independence(x1, x2, all_variables, method, true_labels=true_labels)\n",
    "        log.append((\"independent_no_backdoor\", res_ind))\n",
    "        if res_ind[\"label\"] == 1:\n",
    "            return {\"relation\": \"independent\", \"confidence\": res_ind[\"yes_prob\"], \"log\": log}\n",
    "\n",
    "        true_labels = true_labels_dict.get(\"latent_confounder_no_backdoor\") if true_labels_dict else None\n",
    "        res_latent = check_latent_confounder(x1, x2, all_variables, method, true_labels=true_labels)\n",
    "        log.append((\"latent_confounder_no_backdoor\", res_latent))\n",
    "        if res_latent[\"label\"] == 1:\n",
    "            return {\"relation\": \"x<->y\", \"confidence\": res_latent[\"yes_prob\"], \"log\": log}\n",
    "\n",
    "        true_labels = true_labels_dict.get(\"x->y_no_backdoor\") if true_labels_dict else None\n",
    "        res_dir = check_causal_direction(x1, x2, all_variables, method, true_labels=true_labels)\n",
    "        log.append((\"x->y_no_backdoor\", res_dir))\n",
    "        if res_dir[\"label\"] == 1:\n",
    "            return {\"relation\": \"x->y\", \"confidence\": res_dir[\"yes_prob\"], \"log\": log}\n",
    "        else:\n",
    "            return {\"relation\": \"y->x\", \"confidence\": res_dir[\"no_prob\"], \"log\": log}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ===================== 基础配置 =====================\n",
    "    # 定义分析的变量对和所有变量\n",
    "    x1 = \"冰淇淋销量\"\n",
    "    x2 = \"溺水人数\"\n",
    "    all_variables = [\"冰淇淋销量\", \"溺水人数\", \"温度\"]\n",
    "    \n",
    "    # JSON数据集路径（确保路径正确）\n",
    "    json_path = \"./causal_500.json\"\n",
    "    \n",
    "    # ===================== 读取真实标签（优先用JSON） =====================\n",
    "    # 从JSON读取对应变量对的真实标签（用于校准）\n",
    "    true_labels_dict = load_true_labels_from_json(\n",
    "        json_path=json_path,\n",
    "        x1=x1,\n",
    "        x2=x2\n",
    "    )\n",
    "    \n",
    "    # 【可选】如果没有JSON数据，取消下面注释，使用手动定义的标签（二选一）\n",
    "    # true_labels_dict = {\n",
    "    #     \"backdoor_path\": [1],\n",
    "    #     \"independent_after_block\": [1],\n",
    "    #     \"latent_confounder_after_block\": [0],\n",
    "    #     \"x->y_after_block\": [0],\n",
    "    #     \"independent_no_backdoor\": [1],\n",
    "    #     \"latent_confounder_no_backdoor\": [0],\n",
    "    #     \"x->y_no_backdoor\": [0]\n",
    "    # }\n",
    "    \n",
    "    # ===================== 选择单个方法运行（核心修改） =====================\n",
    "    # 仅使用一个方法（可选：token_prob/frequency/logit，推荐token_prob）\n",
    "    method = \"token_prob\"\n",
    "    \n",
    "    # ===================== 执行因果推断分析 =====================\n",
    "    print(\"开始因果推断分析（本地白盒模型 + token概率 + 温度缩放校准）\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\n使用方法: {method}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # 执行树状推理（传入真实标签校准）\n",
    "    result = tree_query(\n",
    "        x1=x1,\n",
    "        x2=x2,\n",
    "        all_variables=all_variables,\n",
    "        method=method,\n",
    "        true_labels_dict=true_labels_dict\n",
    "    )\n",
    "    \n",
    "    # ===================== 打印结果（简化，仅展示单个方法） =====================\n",
    "    print(f\"\\n=== {method}方法最终结果 ===\")\n",
    "    print(f\"变量对: {x1} ↔ {x2}\")\n",
    "    print(f\"因果关系: {result['relation']}\")\n",
    "    print(f\"判断置信度: {result['confidence']:.4f}\")\n",
    "    \n",
    "    # 详细执行日志（保留完整信息）\n",
    "    print(f\"\\n=== 详细执行日志 ===\")\n",
    "    for step_name, step_result in result[\"log\"]:\n",
    "        print(f\"\\n🔍 步骤: {step_name}\")\n",
    "        print(f\"  最终判断: {'是（Yes）' if step_result['label'] == 1 else '否（No）'}\")\n",
    "        print(f\"  原始P(Yes): {step_result.get('yes_prob', step_result['prob']):.4f}\")\n",
    "        print(f\"  原始P(No): {step_result.get('no_prob', 1-step_result['prob']):.4f}\")\n",
    "        print(f\"  校准温度: {step_result.get('calibration_temp', 1.0):.2f}\")\n",
    "        print(f\"  平均熵值: {step_result.get('avg_entropy', 0.0):.4f}\")\n",
    "        \n",
    "        # 专家详情（保留）\n",
    "        if \"expert_results\" in step_result:\n",
    "            print(f\"  🧑‍🏫 参与专家详情:\")\n",
    "            for expert_result in step_result[\"expert_results\"]:\n",
    "                expert_name = expert_result.get(\"expert\", \"unknown\")\n",
    "                yes_prob = expert_result.get(\"yes_prob\", expert_result[\"prob\"] if expert_result[\"label\"] == 1 else 1 - expert_result[\"prob\"])\n",
    "                no_prob = expert_result.get(\"no_prob\", 1 - yes_prob)\n",
    "                calib_yes = expert_result.get(\"calibrated_yes_prob\", yes_prob)\n",
    "                calib_no = expert_result.get(\"calibrated_no_prob\", no_prob)\n",
    "                entropy = expert_result.get(\"entropy\", 0.0)\n",
    "                print(f\"    - {expert_name}: 原始(Yes={yes_prob:.4f}, No={no_prob:.4f}) → 校准后(Yes={calib_yes:.4f}, No={calib_no:.4f})，熵值={entropy:.4f}\")\n",
    "    \n",
    "    # ===================== 收尾 =====================\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"✅ 分析完成！\")\n",
    "    # 清理模型释放内存\n",
    "    client.unload_model()\n",
    "    print(\"🗑️ 模型已卸载，资源释放完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f978f349",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def compute_all_causal_relations(variables, method='probability'):\n",
    "    \"\"\"\n",
    "    计算图中每两个变量之间的因果关系，使用tree_query函数。\n",
    "    \n",
    "    输出:\n",
    "        {\n",
    "            (x1, x2): {\n",
    "                'relation': 'x->y' | 'y->x' | 'x<->y' | 'independent',\n",
    "                'confidence': float,\n",
    "                'log': [(step_name, {'label': int, 'prob': float}), ...]\n",
    "            },\n",
    "            ...\n",
    "        }\n",
    "    \"\"\"\n",
    "    all_relations = {}\n",
    "\n",
    "    # 生成所有变量的组合 C(n, 2)\n",
    "    for x1, x2 in combinations(variables, 2):\n",
    "        # 进行 tree_query\n",
    "        result = tree_query(x1, x2, method)\n",
    "        \n",
    "        # 存储结果\n",
    "        all_relations[(x1, x2)] = result\n",
    "\n",
    "    return all_relations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6066714d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compute_all_causal_relations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m variables = [\u001b[33m'\u001b[39m\u001b[33m气温\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m冰淇淋销量\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m溺水人数\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m relations = \u001b[43mcompute_all_causal_relations\u001b[49m(variables)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (x1, x2), relation \u001b[38;5;129;01min\u001b[39;00m relations.items():\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRelation between \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx1\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx2\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelation[\u001b[33m'\u001b[39m\u001b[33mrelation\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (Confidence: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelation[\u001b[33m'\u001b[39m\u001b[33mconfidence\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'compute_all_causal_relations' is not defined"
     ]
    }
   ],
   "source": [
    "variables = ['气温', '冰淇淋销量', '溺水人数']\n",
    "relations = compute_all_causal_relations(variables)\n",
    "\n",
    "for (x1, x2), relation in relations.items():\n",
    "    print(f\"Relation between {x1} and {x2}: {relation['relation']} (Confidence: {relation['confidence']})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
