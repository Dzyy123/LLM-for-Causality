# whitebox_calibration.pyï¼ˆè¡¥å……NLLï¼‰
import torch
import numpy as np
from sklearn.metrics import mean_squared_error

def temperature_scaling(probs: np.ndarray, temperature: float = 1.0) -> np.ndarray:
    """
    æ¸©åº¦ç¼©æ”¾æ ¡å‡†ï¼šè°ƒæ•´æ¦‚ç‡åˆ†å¸ƒçš„é”åº¦
    - probs: åŸå§‹æ¦‚ç‡æ•°ç»„ï¼ˆå¦‚[0.8, 0.2]ï¼‰
    - temperature: æ¸©åº¦ç³»æ•°ï¼ˆ<1é”åŒ–ï¼Œ>1å¹³æ»‘ï¼‰
    è¿”å›æ ¡å‡†åçš„æ¦‚ç‡ï¼ˆå½’ä¸€åŒ–ï¼‰
    """
    # é¿å…log(0)ï¼ŒåŠ æå°å€¼
    probs = np.clip(probs, 1e-10, 1.0)
    # è½¬æ¢ä¸ºlogits
    logits = np.log(probs)
    # æ¸©åº¦ç¼©æ”¾
    scaled_logits = logits / temperature
    # è½¬å›æ¦‚ç‡
    exp_logits = np.exp(scaled_logits)
    calibrated_probs = exp_logits / np.sum(exp_logits)
    return calibrated_probs

def calculate_mse(true_probs: np.ndarray, pred_probs: np.ndarray) -> float:
    """è®¡ç®—çœŸå®æ¦‚ç‡ä¸é¢„æµ‹æ¦‚ç‡çš„MSEï¼ˆå‡æ–¹è¯¯å·®ï¼‰"""
    return mean_squared_error(true_probs.flatten(), pred_probs.flatten())

def calculate_nll(true_labels: np.ndarray, pred_probs: np.ndarray) -> float:
    """è®¡ç®—è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼ˆNegative Log-Likelihoodï¼‰ï¼šè¶Šå°è¡¨ç¤ºé¢„æµ‹è¶Šå‡†ï¼ˆå¯¹åº”ä½ çš„â€œè´Ÿå¯¹æ•°å‡½æ•°â€ï¼‰"""
    # é¿å…log(0)ï¼ŒåŠ æå°å€¼
    pred_probs = np.clip(pred_probs, 1e-10, 1.0)
    # äºŒåˆ†ç±»ï¼šå–çœŸå®æ ‡ç­¾å¯¹åº”çš„æ¦‚ç‡çš„å¯¹æ•°
    log_probs = np.log(pred_probs[np.arange(len(true_labels)), true_labels.astype(int)])
    nll = -np.mean(log_probs)
    return nll

def calculate_entropy(probs: np.ndarray) -> float:
    """è®¡ç®—æ¦‚ç‡åˆ†å¸ƒçš„ç†µå€¼ï¼ˆè¡¡é‡ä¸ç¡®å®šæ€§ï¼Œç†µè¶Šå°è¶Šç¡®å®šï¼‰"""
    probs = np.clip(probs, 1e-10, 1.0)
    entropy = -np.sum(probs * np.log2(probs)) / len(probs)
    return entropy

def calibrate_expert_prob(yes_prob: float, no_prob: float, opt_temp: float = 1.0) -> tuple:
    """
    æ ¡å‡†å•ä¸ªä¸“å®¶çš„Yes/Noæ¦‚ç‡
    è¿”å›ï¼š(æ ¡å‡†åçš„yes_prob, æ ¡å‡†åçš„no_prob, ç†µå€¼)
    """
    # æ„é€ æ¦‚ç‡æ•°ç»„
    raw_probs = np.array([yes_prob, no_prob])
    # æ¸©åº¦ç¼©æ”¾æ ¡å‡†
    calibrated_probs = temperature_scaling(raw_probs, opt_temp)
    # è®¡ç®—ç†µå€¼
    entropy = calculate_entropy(calibrated_probs)
    return calibrated_probs[0], calibrated_probs[1], entropy

def find_optimal_temperature(
    expert_probs_list: list, 
    true_labels: list = None,
    optimize_target: str = "nll"  # å¯é€‰ï¼š"mse"ï¼ˆå‡æ–¹è¯¯å·®ï¼‰/ "nll"ï¼ˆè´Ÿå¯¹æ•°ä¼¼ç„¶ï¼‰
) -> float:
    """
    éå†æ¸©åº¦èŒƒå›´ï¼Œæ‰¾åˆ°MSE/NLLæœ€å°çš„æœ€ä¼˜æ¸©åº¦ï¼ˆåŸºäºcausal_500.jsonçš„çœŸå®æ ‡ç­¾ï¼‰
    - expert_probs_list: æ‰€æœ‰æ ·æœ¬çš„åŸå§‹Yesæ¦‚ç‡åˆ—è¡¨ï¼ˆæ¥è‡ªcausal_500.jsonçš„æ¯ä¸ªæ ·æœ¬ï¼‰
    - true_labels: æ‰€æœ‰æ ·æœ¬çš„çœŸå®æ ‡ç­¾ï¼ˆ1=Yesï¼Œ0=Noï¼‰ï¼ˆæ¥è‡ªcausal_500.jsonçš„causalå­—æ®µï¼‰
    - optimize_target: ä¼˜åŒ–ç›®æ ‡ï¼ˆmse/nllï¼‰
    """
    if true_labels is None or len(expert_probs_list) != len(true_labels):
        return 1.0  # æ— çœŸå®æ ‡ç­¾æ—¶ç”¨é»˜è®¤æ¸©åº¦
    
    # éå†æ¸©åº¦èŒƒå›´ï¼ˆå¯è°ƒæ•´ï¼‰
    temp_range = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.2, 1.5, 2.0, 2.5, 3.0]
    loss_list = []

    for temp in temp_range:
        # æ ¡å‡†æ‰€æœ‰æ ·æœ¬çš„æ¦‚ç‡
        calibrated_probs = []
        for yes_prob in expert_probs_list:
            raw_probs = np.array([yes_prob, 1-yes_prob])
            calib_probs = temperature_scaling(raw_probs, temp)
            calibrated_probs.append(calib_probs[0])  # å–Yesçš„æ¦‚ç‡
        
        # è®¡ç®—å½“å‰æ¸©åº¦ä¸‹çš„æŸå¤±ï¼ˆMSEæˆ–NLLï¼‰
        if optimize_target == "mse":
            loss = calculate_mse(np.array(true_labels), np.array(calibrated_probs))
        elif optimize_target == "nll":
            # NLLéœ€è¦æ„é€ äºŒåˆ†ç±»æ¦‚ç‡çŸ©é˜µ + çœŸå®æ ‡ç­¾ç´¢å¼•
            true_labels_np = np.array(true_labels)
            calib_probs_np = np.array([[p, 1-p] for p in calibrated_probs])
            loss = calculate_nll(true_labels_np, calib_probs_np)
        else:
            loss = calculate_mse(np.array(true_labels), np.array(calibrated_probs))
        
        loss_list.append(loss)
    
    # æ‰¾æŸå¤±æœ€å°çš„æ¸©åº¦ï¼ˆæœ€ä¼˜æ¸©åº¦ï¼‰
    opt_temp = temp_range[np.argmin(loss_list)]
    # å¯é€‰ï¼šæ‰“å°å„æ¸©åº¦çš„æŸå¤±ï¼ˆä¾¿äºè°ƒè¯•ï¼‰
    print(f"ã€æœ€ä¼˜æ¸©åº¦è®¡ç®—ã€‘æ¸©åº¦èŒƒå›´: {temp_range}")
    print(f"ã€æœ€ä¼˜æ¸©åº¦è®¡ç®—ã€‘{optimize_target}åˆ—è¡¨: {[round(l, 4) for l in loss_list]}")
    print(f"ã€æœ€ä¼˜æ¸©åº¦è®¡ç®—ã€‘æœ€ä¼˜æ¸©åº¦: {opt_temp} (æœ€å°{optimize_target}: {min(loss_list):.4f})")
    return opt_temp

def dynamic_calibrate(
    yes_prob: float,
    no_prob: float,
    performance_stats: dict,
    base_temp: float = 1.0
) -> tuple:
    """
    åŸºäºç›¸ä¼¼æ ·æœ¬è¡¨ç°åŠ¨æ€æ ¡å‡†ç½®ä¿¡åº¦
    æ ¸å¿ƒé€»è¾‘ï¼š
    - ç›¸ä¼¼æ ·æœ¬æç«¯é”™è¯¯ç‡è¶Šé«˜ â†’ æ¸©åº¦è¶Šé«˜ï¼ˆç½®ä¿¡åº¦ä¸‹è°ƒè¶Šå¤šï¼‰
    - ç›¸ä¼¼æ ·æœ¬æç«¯æ­£ç¡®ç‡è¶Šé«˜ â†’ æ¸©åº¦è¶Šä½ï¼ˆç½®ä¿¡åº¦å¾®è°ƒ/ä¸è°ƒï¼‰
    - æ— ç›¸ä¼¼æ ·æœ¬ â†’ ç”¨åŸºç¡€æ¸©åº¦
    """
    extreme_error_ratio = performance_stats["extreme_error_ratio"]
    extreme_correct_ratio = performance_stats["extreme_correct_ratio"]
    avg_similarity = performance_stats["avg_similarity"]
    
    # åŠ¨æ€è®¡ç®—æ¸©åº¦ç³»æ•°ï¼ˆç›¸ä¼¼åº¦è¶Šé«˜ï¼Œè°ƒæ•´å¹…åº¦è¶Šå¤§ï¼‰
    temp_adjust_coeff = extreme_error_ratio * 2.0  # æç«¯é”™è¯¯ç‡æœ€é«˜è®©æ¸©åº¦+2
    temp_adjust_coeff -= extreme_correct_ratio * 0.5  # æç«¯æ­£ç¡®ç‡æœ€é«˜è®©æ¸©åº¦-0.5
    temp_adjust_coeff *= avg_similarity  # ç›¸ä¼¼åº¦åŠ æƒï¼ˆä½ç›¸ä¼¼åˆ™è°ƒæ•´å¹…åº¦å°ï¼‰
    
    # æœ€ç»ˆæ¸©åº¦ï¼ˆé™åˆ¶èŒƒå›´ï¼š0.8~3.0ï¼Œé¿å…æç«¯ï¼‰
    final_temp = base_temp + temp_adjust_coeff
    final_temp = np.clip(final_temp, 0.8, 3.0)
    
    # æ¸©åº¦ç¼©æ”¾æ ¡å‡†
    raw_probs = np.array([yes_prob, no_prob])
    calibrated_probs = temperature_scaling(raw_probs, final_temp)
    calib_entropy = calculate_entropy(calibrated_probs)
    
    # è¾“å‡ºæ ¡å‡†é€»è¾‘è§£é‡Š
    print(f"\nğŸ”§ åŠ¨æ€æ ¡å‡†é€»è¾‘ï¼š")
    print(f"  ç›¸ä¼¼æ ·æœ¬æç«¯é”™è¯¯ç‡ï¼š{extreme_error_ratio:.2%} â†’ æ¸©åº¦+{temp_adjust_coeff:.2f}")
    print(f"  ç›¸ä¼¼æ ·æœ¬æç«¯æ­£ç¡®ç‡ï¼š{extreme_correct_ratio:.2%} â†’ æ¸©åº¦-{extreme_correct_ratio*0.5:.2f}")
    print(f"  å¹³å‡ç›¸ä¼¼åº¦ï¼š{avg_similarity:.4f} â†’ è°ƒæ•´å¹…åº¦åŠ æƒ")
    print(f"  æœ€ç»ˆæ ¡å‡†æ¸©åº¦ï¼š{final_temp:.2f}ï¼ˆåŸºç¡€æ¸©åº¦ï¼š{base_temp}ï¼‰")
    
    return calibrated_probs[0], calibrated_probs[1], calib_entropy, final_temp


def identify_extreme_error_samples(expert_probs_list: list, true_labels: list) -> dict:
    """Identify extreme error/correct samples in test set (è¾…åŠ©å‡½æ•°)"""
    extreme_error_samples = []  # Extreme error: probâ‰¥0.9 but label=0, or probâ‰¤0.1 but label=1
    extreme_correct_samples = []# Extreme correct: probâ‰¥0.9 and label=1, or probâ‰¤0.1 and label=0
    normal_samples = []         # Normal samples: prob 0.1~0.9
    
    for idx, (prob, label) in enumerate(zip(expert_probs_list, true_labels)):
        if (prob >= 0.9 and label == 0) or (prob <= 0.1 and label == 1):
            extreme_error_samples.append({"index": idx, "prob": prob, "label": label})
        elif (prob >= 0.9 and label == 1) or (prob <= 0.1 and label == 0):
            extreme_correct_samples.append({"index": idx, "prob": prob, "label": label})
        else:
            normal_samples.append({"index": idx, "prob": prob, "label": label})
    
    return {
        "extreme_error_samples": extreme_error_samples,
        "extreme_correct_samples": extreme_correct_samples,
        "normal_samples": normal_samples,
        "stats": {
            "extreme_error_ratio": len(extreme_error_samples)/len(expert_probs_list),
            "extreme_correct_ratio": len(extreme_correct_samples)/len(expert_probs_list),
            "normal_ratio": len(normal_samples)/len(expert_probs_list)
        }
    }

def calculate_entropy(probs: np.ndarray) -> float:
    """Calculate entropy (uncertainty) of probability distribution (è¾…åŠ©å‡½æ•°)"""
    probs = probs[probs > 0]  # Avoid log(0)
    return -np.sum(probs * np.log2(probs))

def temperature_scaling_base(probs: np.ndarray, temperature: float = 1.0) -> np.ndarray:
    """åŸºç¡€æ¸©åº¦ç¼©æ”¾ï¼ˆè¾…åŠ©å‡½æ•°ï¼‰"""
    probs = probs / temperature
    probs = np.exp(probs) / np.sum(np.exp(probs))
    return probs

# ===================== æ–¹æ³•1ï¼šåŠ æƒè¯¯å·®ä¿®æ­£æ³• =====================
def weighted_error_calibration(
    new_init_prob: float,  
    similar_samples: list, 
    test_embedding_db: dict 
) -> float:
    """
    åŠ æƒè¯¯å·®ä¿®æ­£æ³•ï¼šç”¨ç›¸ä¼¼æ ·æœ¬çš„è¯¯å·®ï¼ˆçœŸå®å€¼-é¢„æµ‹å€¼ï¼‰åŠ æƒä¿®æ­£åˆå§‹æ¦‚ç‡
    """
    if not similar_samples:
        print("  âš ï¸ No similar samples, return original probability")
        return new_init_prob
    
    # æå–ç›¸ä¼¼æ ·æœ¬æ•°æ®
    sim_sample_ids = [s["sample_id"] for s in similar_samples]
    sim_scores = [s["similarity"] for s in similar_samples]
    sim_errors = test_embedding_db["sample_errors"][sim_sample_ids]
    
    # åŠ æƒå¹³å‡è¯¯å·®
    total_sim = sum(sim_scores)
    if total_sim == 0:
        return new_init_prob
    weighted_error = sum([s * e for s, e in zip(sim_scores, sim_errors)]) / total_sim
    
    # æ ¡å‡†å¹¶è£å‰ª
    calibrated_prob = np.clip(new_init_prob + weighted_error, 0.0, 1.0)
    
    # è¾“å‡ºè¿‡ç¨‹
    print(f"\nğŸ” [Weighted Error Correction]")
    print(f"  Original Prob: {new_init_prob:.4f} | Weighted Error: {weighted_error:.4f} | Calibrated Prob: {calibrated_prob:.4f}")
    return calibrated_prob

# ===================== æ–¹æ³•2ï¼šç›¸ä¼¼æ ·æœ¬ç½®ä¿¡åº¦èåˆæ³• =====================
def similarity_confidence_fusion(
    new_init_prob: float,
    similar_samples: list,
    test_embedding_db: dict
) -> float:
    """
    ä¼˜åŒ–ç‰ˆï¼šè¿‡æ»¤æç«¯é”™è¯¯æ ·æœ¬ + åŠ¨æ€èåˆç³»æ•° + è¯¯å·®ä¿®æ­£å…œåº•
    """
    if not similar_samples:
        print("  âš ï¸ No similar samples, return original probability")
        return new_init_prob
    
    # ========== æ­¥éª¤1ï¼šè¿‡æ»¤æç«¯é”™è¯¯æ ·æœ¬ï¼ˆåªä¿ç•™æ­£ç¡®/æ­£å¸¸æ ·æœ¬ï¼‰ ==========
    # ç­›é€‰æ¡ä»¶ï¼šæ’é™¤ performance=0ï¼ˆExtreme Errorï¼‰çš„æ ·æœ¬
    valid_similar_samples = [s for s in similar_samples if s["performance"] != 0]
    if not valid_similar_samples:
        print("  âš ï¸ All similar samples are Extreme Error! Use error correction instead.")
        # å…œåº•ï¼šæ”¹ç”¨åŠ æƒè¯¯å·®ä¿®æ­£ï¼ˆç”¨è¯¯å·®è€Œéé¢„æµ‹æ¦‚ç‡ï¼‰
        sim_sample_ids = [s["sample_id"] for s in similar_samples]
        sim_scores = [s["similarity"] for s in similar_samples]
        sim_errors = test_embedding_db["sample_errors"][sim_sample_ids]
        total_sim = sum(sim_scores)
        weighted_error = sum([s * e for s, e in zip(sim_scores, sim_errors)]) / total_sim if total_sim > 0 else 0
        calibrated_prob = np.clip(new_init_prob + weighted_error, 0.0, 1.0)
        print(f"  Error-corrected Prob: {calibrated_prob:.4f}")
        return calibrated_prob
    
    # ========== æ­¥éª¤2ï¼šæå–æœ‰æ•ˆæ ·æœ¬çš„ç›¸ä¼¼åº¦å’Œé¢„æµ‹æ¦‚ç‡ ==========
    sim_sample_ids = [s["sample_id"] for s in valid_similar_samples]
    sim_scores = [s["similarity"] for s in valid_similar_samples]
    sim_probs = test_embedding_db["sample_pred_probs"][sim_sample_ids]
    
    # ========== æ­¥éª¤3ï¼šåŠ¨æ€è®¡ç®—èåˆç³»æ•°ï¼ˆæ ¹æ®æœ‰æ•ˆæ ·æœ¬çš„æ­£ç¡®ç‡ï¼‰ ==========
    # è®¡ç®—æœ‰æ•ˆæ ·æœ¬çš„æ­£ç¡®ç‡ï¼ˆperformance=1çš„æ ·æœ¬å æ¯”ï¼‰
    correct_count = len([s for s in valid_similar_samples if s["performance"] == 1])
    valid_total = len(valid_similar_samples)
    correct_ratio = correct_count / valid_total if valid_total > 0 else 0.0
    
    # åŠ¨æ€èåˆç³»æ•°ï¼šæ­£ç¡®ç‡è¶Šé«˜ï¼Œèåˆç³»æ•°è¶Šå¤§ï¼ˆèŒƒå›´ï¼š0.1~0.8ï¼‰
    fusion_coeff = max(0.1, min(0.8, correct_ratio))  # é¿å…ç³»æ•°è¿‡0æˆ–è¿‡1
    original_coeff = 1 - fusion_coeff
    
    # ========== æ­¥éª¤4ï¼šæœ‰æ•ˆæ ·æœ¬çš„ç›¸ä¼¼åº¦åŠ æƒèåˆ ==========
    total_sim = sum(sim_scores)
    fused_prob = sum([s * p for s, p in zip(sim_scores, sim_probs)]) / total_sim if total_sim > 0 else new_init_prob
    
    # ========== æ­¥éª¤5ï¼šåŠ¨æ€åŠ æƒèåˆï¼ˆåŸå§‹æ¦‚ç‡ + æœ‰æ•ˆæ ·æœ¬èåˆæ¦‚ç‡ï¼‰ ==========
    calibrated_prob = np.clip(
        original_coeff * new_init_prob + fusion_coeff * fused_prob,
        0.0, 1.0
    )
    
    # è¾“å‡ºä¼˜åŒ–åçš„è¿‡ç¨‹
    print(f"\nğŸ” [Optimized Similarity Confidence Fusion]")
    print(f"  Valid Similar Samples: {valid_total}/{len(similar_samples)} (filtered Extreme Error)")
    print(f"  Similar Samples Correct Ratio: {correct_ratio:.2%} â†’ Fusion Coeff: {fusion_coeff:.2f}")
    print(f"  Original Prob: {new_init_prob:.4f} | Fused Prob (Valid Samples): {fused_prob:.4f}")
    print(f"  Calibrated Prob: {calibrated_prob:.4f} (original_coeff={original_coeff:.2f}, fusion_coeff={fusion_coeff:.2f})")
    
    return calibrated_prob

# ===================== æ–¹æ³•3ï¼šä¸ç¡®å®šæ€§åŠ æƒæ ¡å‡†æ³• =====================
def uncertainty_weighted_calibration(
    new_init_prob: float,
    similar_samples: list,
    test_embedding_db: dict
) -> float:
    """
    ä¸ç¡®å®šæ€§åŠ æƒæ ¡å‡†æ³•ï¼šç»“åˆç›¸ä¼¼æ ·æœ¬çš„ç†µï¼ˆä¸ç¡®å®šæ€§ï¼‰ï¼Œé«˜ä¸ç¡®å®šæ€§æ ·æœ¬èµ‹äºˆæ›´é«˜çš„è¯¯å·®ä¿®æ­£æƒé‡
    """
    if not similar_samples:
        print("  âš ï¸ No similar samples, return original probability")
        return new_init_prob
    
    # æå–ç›¸ä¼¼æ ·æœ¬æ•°æ®
    sim_sample_ids = [s["sample_id"] for s in similar_samples]
    sim_scores = [s["similarity"] for s in similar_samples]
    sim_errors = test_embedding_db["sample_errors"][sim_sample_ids]
    
    # è®¡ç®—ç›¸ä¼¼æ ·æœ¬çš„ç†µï¼ˆä¸ç¡®å®šæ€§ï¼‰
    sim_probs = test_embedding_db["sample_pred_probs"][sim_sample_ids]
    sim_entropies = [calculate_entropy(np.array([p, 1-p])) for p in sim_probs]
    
    # ä¸ç¡®å®šæ€§åŠ æƒï¼ˆç†µè¶Šé«˜ï¼Œæƒé‡è¶Šå¤§ï¼‰
    entropy_weights = [e / sum(sim_entropies) if sum(sim_entropies) > 0 else 1/len(sim_entropies) for e in sim_entropies]
    weighted_sim_scores = [s * w for s, w in zip(sim_scores, entropy_weights)]
    
    # åŠ æƒè¯¯å·®ä¿®æ­£
    total_weighted_sim = sum(weighted_sim_scores)
    if total_weighted_sim == 0:
        return new_init_prob
    weighted_error = sum([s * e for s, e in zip(weighted_sim_scores, sim_errors)]) / total_weighted_sim
    
    # æ ¡å‡†å¹¶è£å‰ª
    calibrated_prob = np.clip(new_init_prob + weighted_error, 0.0, 1.0)
    
    # è¾“å‡ºè¿‡ç¨‹
    print(f"\nğŸ” [Uncertainty Weighted Calibration]")
    print(f"  Original Prob: {new_init_prob:.4f} | Uncertainty-Weighted Error: {weighted_error:.4f} | Calibrated Prob: {calibrated_prob:.4f}")
    return calibrated_prob

# ===================== æ–¹æ³•4ï¼šåˆ†ä½æ•°æ ¡å‡†æ³• =====================
def quantile_calibration(
    new_init_prob: float,
    similar_samples: list,
    test_embedding_db: dict,
    quantile: float = 0.95  # åˆ†ä½æ•°ï¼ˆå¦‚0.95è¡¨ç¤ºå–95%åˆ†ä½æ•°ï¼‰
) -> float:
    """
    åˆ†ä½æ•°æ ¡å‡†æ³•ï¼šåŸºäºæµ‹è¯•é›†è¯¯å·®çš„åˆ†ä½æ•°ï¼Œé™åˆ¶æç«¯è¯¯å·®å½±å“ï¼Œè°ƒæ•´æ–°æ ·æœ¬æ¦‚ç‡
    """
    if not similar_samples:
        print("  âš ï¸ No similar samples, return original probability")
        return new_init_prob
    
    # æå–ç›¸ä¼¼æ ·æœ¬è¯¯å·®
    sim_sample_ids = [s["sample_id"] for s in similar_samples]
    sim_errors = test_embedding_db["sample_errors"][sim_sample_ids]
    
    # è®¡ç®—è¯¯å·®çš„åˆ†ä½æ•°ï¼Œé™åˆ¶æç«¯è¯¯å·®
    error_upper = np.quantile(sim_errors, quantile)
    error_lower = np.quantile(sim_errors, 1 - quantile)
    clipped_errors = [np.clip(e, error_lower, error_upper) for e in sim_errors]
    
    # å¹³å‡ä¿®æ­£è¯¯å·®
    avg_clipped_error = np.mean(clipped_errors)
    calibrated_prob = np.clip(new_init_prob + avg_clipped_error, 0.0, 1.0)
    
    # è¾“å‡ºè¿‡ç¨‹
    print(f"\nğŸ” [Quantile Calibration (q={quantile})]")
    print(f"  Original Prob: {new_init_prob:.4f} | Clipped Avg Error: {avg_clipped_error:.4f} | Calibrated Prob: {calibrated_prob:.4f}")
    return calibrated_prob

# ===================== æ–¹æ³•5ï¼šä¸ªæ€§åŒ–æ¸©åº¦ç¼©æ”¾ =====================
def personalized_temperature_scaling(
    new_init_prob: float,
    similar_samples: list,
    test_embedding_db: dict
) -> float:
    """
    ä¸ªæ€§åŒ–æ¸©åº¦ç¼©æ”¾ï¼šåŸºäºç›¸ä¼¼æ ·æœ¬çš„æ¦‚ç‡åˆ†å¸ƒï¼ŒåŠ¨æ€è®¡ç®—æ¸©åº¦ç³»æ•°ï¼Œç¼©æ”¾æ–°æ ·æœ¬æ¦‚ç‡
    """
    if not similar_samples:
        print("  âš ï¸ No similar samples, return original probability (temp=1.0)")
        return new_init_prob
    
    # æå–ç›¸ä¼¼æ ·æœ¬çš„é¢„æµ‹æ¦‚ç‡
    sim_sample_ids = [s["sample_id"] for s in similar_samples]
    sim_probs = test_embedding_db["sample_pred_probs"][sim_sample_ids]
    sim_true_labels = test_embedding_db["sample_true_labels"][sim_sample_ids]
    
    # è®¡ç®—ç›¸ä¼¼æ ·æœ¬çš„æ¸©åº¦ç³»æ•°ï¼ˆæœ€å°åŒ–äº¤å‰ç†µï¼‰
    def cross_entropy_loss(temp):
        scaled_probs = temperature_scaling_base(np.array([[p, 1-p] for p in sim_probs]), temp)
        ce = -np.sum([sim_true_labels[i] * np.log(scaled_probs[i][0]) + (1 - sim_true_labels[i]) * np.log(scaled_probs[i][1]) for i in range(len(sim_probs))])
        return ce
    
    # ä¼˜åŒ–æ¸©åº¦ç³»æ•°ï¼ˆèŒƒå›´ï¼š0.1~10.0ï¼‰
    from scipy.optimize import minimize_scalar
    res = minimize_scalar(cross_entropy_loss, bounds=(0.1, 10.0), method='bounded')
    optimal_temp = res.x if res.success else 1.0
    
    # åº”ç”¨æ¸©åº¦ç¼©æ”¾
    scaled_probs = temperature_scaling_base(np.array([new_init_prob, 1 - new_init_prob]), optimal_temp)
    calibrated_prob = scaled_probs[0]
    
    # è¾“å‡ºè¿‡ç¨‹
    print(f"\nğŸ” [Personalized Temperature Scaling]")
    print(f"  Original Prob: {new_init_prob:.4f} | Optimal Temp: {optimal_temp:.2f} | Calibrated Prob: {calibrated_prob:.4f}")
    return calibrated_prob

# ===================== æ ¡å‡†æ–¹æ³•é€‰æ‹©å™¨ =====================
def calibrate_probability(
    method: str,
    new_init_prob: float,
    similar_samples: list,
    test_embedding_db: dict,
    **kwargs
) -> float:
    """
    æ ¡å‡†æ–¹æ³•ç»Ÿä¸€å…¥å£ï¼šé€šè¿‡methodå‚æ•°é€‰æ‹©ä½¿ç”¨å“ªç§æ ¡å‡†æ–¹æ³•
    :param method: å¯é€‰å€¼ï¼šweighted_error / similarity_fusion / uncertainty_weighted / quantile / personalized_temp
    :param new_init_prob: æ–°æ ·æœ¬åˆå§‹æ¦‚ç‡
    :param similar_samples: ç›¸ä¼¼æ ·æœ¬åˆ—è¡¨
    :param test_embedding_db: æµ‹è¯•é›†å‘é‡åº“
    :param kwargs: å…¶ä»–å‚æ•°ï¼ˆå¦‚quantileæ ¡å‡†çš„quantileå€¼ï¼‰
    :return: æ ¡å‡†åçš„æ¦‚ç‡
    """
    method_map = {
        "weighted_error": weighted_error_calibration,
        "similarity_fusion": similarity_confidence_fusion,
        "uncertainty_weighted": uncertainty_weighted_calibration,
        "quantile": quantile_calibration,
        "personalized_temp": personalized_temperature_scaling
    }
    
    if method not in method_map:
        raise ValueError(f"âŒ Invalid calibration method: {method}\nSupported methods: {list(method_map.keys())}")
    
    # è°ƒç”¨å¯¹åº”æ ¡å‡†æ–¹æ³•
    if method == "quantile":
        quantile = kwargs.get("quantile", 0.95)
        return method_map[method](new_init_prob, similar_samples, test_embedding_db, quantile=quantile)
    else:
        return method_map[method](new_init_prob, similar_samples, test_embedding_db)